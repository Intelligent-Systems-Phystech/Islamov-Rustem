
@Article{L-Newton2019,
  author  = {Polyak, Boris and Tremba, Andrey},
  journal = {arXiv preprint arXiv:1703.07810},
  title   = {New versions of {N}ewton method: step-size choice, convergence domain and under-determined equations},
  year    = {2019},
}


@article{gould2010solving,
	title={On solving trust-region and other regularised subproblems in optimization},
	author={Gould, Nicholas IM and Robinson, Daniel P and Thorne, H Sue},
	journal={Mathematical Programming Computation},
	volume={2},
	number={1},
	pages={21--57},
	year={2010},
	publisher={Springer}
}

@conference{KFJ,
title = {Distributed learning with compressed gradients},
author = {Khirirat, Sarit and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
booktitle = {arXiv preprint arXiv:1806.06573},
pages = {},
year = {2018}
}

@inproceedings{Tang19,
	author    = "H. Tang and X. Lian and T. Zhang and J. Liu",
	title     = "{D}ouble{S}queeze: Parallel Stochastic Gradient Descent with Double-Pass Error-Compensated Compression",
	booktitle  = "Proceedings of the 36th International Conference on Machine Learning",
	year      = "2019",
	pages     = "6155--6165",
}


@Article{Stich19,
	author = 	 "S. U. Stich and S. P. Karimireddy",
	title = 	 "The error-feedback framework: Better rates for {SGD} with delayed gradients and compressed communication",
	journal =	 "arXiv: 1909.05350",
	year =	 "2019"
}

@Article{Wen17,
	author = 	 "W. Wen and C. Xu and F. Yan and C. Wu and Y. Wang and H. Li",
	title = 	 "Terngrad: Ternary gradients to reduce communication in distributed deep learning",
	journal =	 "Advances in Neural Information Processing Systems",
	year =	 "2017",
	pages =	 "1509--1519"
}

@Article{Seide14,
	author = 	 "F. Seide and H. Fu and J. Droppo and G. Li and D. Yu",
	title = 	 "1-bit stochastic gradient descent and its application to data- parallel distributed training of speech {DNN}s",
	journal =	 "Fifteenth Annual Conference of the International Speech Communication Association",
	year =	 "2014"
}


@book{bekkerman2011scaling,
	title={Scaling up machine learning: Parallel and distributed approaches},
	author={Bekkerman, Ron and Bilenko, Mikhail and Langford, John},
	year={2011},
	publisher={Cambridge University Press}
}

@article{polyak2020new,
	title={New versions of Newton method: step-size choice, convergence domain and under-determined equations},
	author={Polyak, Boris and Tremba, Andrey},
	journal={Optimization Methods and Software},
	volume={35},
	number={6},
	pages={1272--1303},
	year={2020},
	publisher={Taylor \& Francis}
}


@article{lin2015universal,
	title={A universal catalyst for first-order optimization},
	author={Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
	journal={arXiv preprint arXiv:1506.02186},
	year={2015}
}


@InProceedings{ADIANA,
  author    = {Zhize Li and Dmitry Kovalev and Xun Qian and Peter Richt\'{a}rik},
  booktitle = {International Conference on Machine Learning},
  title     = {Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization},
  year      = {2020},
}

@Article{DIANA2,
  author  = {Horv\'{a}th, Samuel and Kovalev, Dmitry and Mishchenko, Konstantin and Stich, Sebastian and Richt\'{a}rik, Peter},
  title   = {Stochastic distributed learning with gradient quantization and variance reduction},
  journal = {arXiv preprint arXiv:1904.05115},
  year    = {2019},
}


@Article{DIANA,
  author  = {Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
  journal = {arXiv preprint arXiv:1901.09269},
  title   = {Distributed Learning with Compressed Gradient Differences},
  year    = {2019},
}


@Article{Cnat,
  author  = {Samuel Horv\'{a}th and Chen-Yu Ho and \v{L}udov\'{i}t Horv\'{a}th and Atal Narayan Sahu and Marco Canini and Peter Richt\'{a}rik},
  title   = {Natural compression for distributed deep learning},
  journal = {arXiv preprint arXiv:1905.10988},
  year    = {2019},
}

@article{Wallis1685,
author = {Wallis, John},
title = {A treatise of algebra, both historical and practical},
journal = {Philosophical Transactions of the Royal Society of London},
volume = {15},
number = {173},
pages = {1095-1106},
year = {1685},
doi = {10.1098/rstl.1685.0053},
URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rstl.1685.0053},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rstl.1685.0053},
}

@article{Raphson1697,
author = {Josepho Raphson},
title = {Analysis aequationum universalis seu ad aequationes algebraicas resolvendas methodus generalis, \& expedita, ex nova infinitarum serierum methodo, deducta ac demonstrata},
publisher = {Thomas Braddyll},
journal = {Oxford: Richard Davis},
year = {1697},
}

@Article{Goldfarb1970,
  Title                    = {A Family of Variable-Metric Methods Derived by Variational Means},
  Author                   = {Goldfarb, Donald},
  Journal                  = {Mathematics of Computation},
  Year                     = {1970},
  Number                   = {109},
  Pages                    = {23--26},
  Volume                   = {24}
}

@Article{shanno1970conditioning,
  Title                    = {Conditioning of quasi-{N}ewton methods for function minimization},
  Author                   = {Shanno, David F},
  Journal                  = {Mathematics of computation},
  Year                     = {1970},
  Number                   = {111},
  Pages                    = {647--656},
  Volume                   = {24}
}

@Article{Fletcher1970,
  Title                    = {A New Approach to Variable Metric Algorithms},
  Author                   = {Fletcher, Rodger},
  Journal                  = {The Computer Journal},
  Year                     = {1970},
  Number                   = {3},
  Pages                    = {317--323},
  Volume                   = {13}
}

  
@article{Broyden1967,
title={Quasi-Newton methods and their application to function minimisation},
author={Broyden, Charles G},
journal={Mathematics of Computation},
volume={21},
number={99},
pages={368--381},
year={1967},
publisher={JSTOR}
}

@book{shai_book,
  title={Understanding machine learning: from theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge University Press},
}

@Article{NeedellWard2015,
  author  = {Needell, Deanna and Srebro, Nathan and Ward, Rachel},
  title   = {Stochastic gradient descent, weighted sampling, and the randomized {K}aczmarz algorithm},
  journal = {Mathematical Programming},
  year    = {2015},
  volume  = {155},
  number  = {1--2},
  pages   = {549--573},
}




@Book{Beck-book-nonlinear,
  author    = {Beck, Amir},
  publisher = {Society for Industrial and Applied Mathematics},
  title     = {Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MATLAB},
  year      = {2014},
  address   = {USA},
  isbn      = {1611973643},
}


@InProceedings{MM2019,
  author    = {Malitsky, Yura and Mishchenko, Konstantin},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  title     = {Adaptive gradient descent without descent},
  year      = {2019},
  editor    = {Hal Daumé III and Aarti Singh},
  month     = {13--18 Jul},
  pages     = {6702--6712},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
}

@InProceedings{RR,
  author    = {Mishchenko, Konstantin and Khaled, Ahmed and Richt\'{a}rik, Peter},
  booktitle = {34th Conference on Neural Information Processing Systems (NeurIPS 2020)},
  title     = {Random Reshuffling: Simple Analysiswith Vast Improvements},
  year      = {2020},
}


@InProceedings{localSGD-AISTATS2020,
	author    = {Khaled, Ahmed and Mishchenko, Konstantin and Richt\'{a}rik, Peter},
	title     = {Tighter theory for local {SGD} on identical and heterogeneous data},
	booktitle = {The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS 2020)},
	year      = {2020},
}



@InProceedings{localSGD-Stich,
	author    = {Stich, Sebastian U.},
	title     = {Local {SGD} Converges Fast and Communicates Little},
	booktitle = {International Conference on Learning Representations},
	year      = {2020},
}


@InProceedings{stich2018sparsified,
  author    = {Stich, Sebastian U. and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Sparsified {SGD} with memory},
  year      = {2018},
  pages     = {4447--4458},
}


@Article{COCOA+journal,
	author  = {Ma, Chenxin and Kone\v{c}n\'{y}, Jakub and Jaggi, Martin and Smith, Virginia and Jordan, Michael I. and Richt\'{a}rik, Peter and Tak\'{a}\v{c}, Martin},
	journal = {Optimization Methods and Software},
	title   = {Distributed optimization with arbitrary local solvers},
	year    = {2017},
	number  = {4},
	pages   = {813--848},
	volume  = {32},
	groups  = {richtap:1},
}



@Article{Bernstein18,
	author    = "J. Bernstein and Y. X. Wang and K. Azizzadenesheli and A. Anandkumar",
	title     = "Sign{SGD}: Compressed optimisation for non-convex problems",
	journal   = "The 35th International Conference on Machine Learning",
	year      = "2018",
	pages     = "560--569",
	booktitle = "Proceedings of the 35th International Conference on Machine Learning, PMLR"
}


@Article{biased2020,
  author  = {Aleksandr Beznosikov and Samuel Horv\'{a}th and Peter Richt\'{a}rik and Mher Safaryan},
  title   = {On Biased Compression for Distributed Learning},
  journal = {arXiv:2002.12410},
  year    = {2020},
}


@Article{Alistarh17,
	author = 	 "D. Alistarh and D. Grubic and J. Li and R. Tomioka and M. Vojnovic",
	title = 	 "{QSGD}: Communication-efficient {SGD} via gradient quantization and encoding",
	journal =	 "Advances in Neural Information Processing Systems",
	year =	 "2017",
	pages =	 "1709--1720"
}

@Article{SMOMENTUM,
	author        = {Loizou, Nicolas and Richt\'{a}rik, Peter},
	title         = {Momentum and stochastic momentum for stochastic gradient, {N}ewton, proximal point and subspace descent methods},
	journal       = {arXiv:1712.09677},
	year          = {2017},
}


@InProceedings{SHB-NIPS,
	author        = {Loizou, Nicolas and Richt\'{a}rik, Peter},
	title         = {Linearly convergent stochastic heavy ball method for minimizing generalization error},
	booktitle     = {NIPS Workshop on Optimization for Machine Learning},
	year          = {2017},
}


@inProceedings{SAGA,
	title = {{SAGA}: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives},
	author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
	booktitle = {Advances in Neural Information Processing Systems 27},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
	pages = {1646--1654},
	year = {2014},
	publisher = {Curran Associates, Inc.},
}

@Article{proxSVRG,
	author  = {Xiao, Lin and Zhang, Tong},
	title   = {A proximal stochastic gradient method with progressive variance reduction},
	journal = {SIAM Journal on Optimization},
	year    = {2014},
	volume  = {24},
	number  = {4},
	pages   = {2057--2075},
}

@InProceedings{johnson2013accelerating,
	Title                    = {Accelerating stochastic gradient descent using predictive variance reduction},
	Author                   = {Johnson, Rie and Zhang, Tong},
	Booktitle                = {NIPS},
	Year                     = {2013},
	Pages                    = {315--323}
}


@Article{schmidt2017minimizing,
	Title                    = {Minimizing finite sums with the stochastic average gradient},
	Author                   = {Schmidt, M. and Le Roux, N. and Bach, F.},
	Journal                  = {Math. Program.},
	Year                     = {2017},
	Number                   = {1-2},
	Pages                    = {83--112},
	Volume                   = {162},	
	Publisher                = {Springer}
}


@Article{IProx-SDCA,
	author    = {Peilin Zhao and Tong Zhang},
	title     = {Stochastic optimization with importance sampling},
	journal   = {The 32nd International Conference on Machine Learning},
	year      = {2015},
	volume    = {37},
	pages     = {1--9},
	booktitle = {Proceedings of the 32nd International Conference on Machine Learning, PMLR},
}






@InProceedings{pegasos2,
	author        = {Tak{\'a}\v{c}, Martin and Bijral, Avleen and Richt{\'a}rik, Peter and Srebro, Nathan},
	title         = {Mini-batch primal and dual methods for {SVM}s},
	booktitle     = {30th International Conference on Machine Learning},
	year          = {2013},
	pages         = {537--552},
}




@Article{Nemirovski-Juditsky-Lan-Shapiro-2009,
	author  = {Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
	title   = {Robust stochastic approximation approach to stochastic programming},
	journal = {SIAM Journal on Optimization},
	year    = {2009},
	volume  = {19},
	number  = {4},
	pages   = {1574--1609},
}


@article{RobbinsMonro:1951,
	added-at = {2008-10-07T16:03:39.000+0200},
	author = {Robbins, H. and Monro, S.},
	biburl = {http://www.bibsonomy.org/bibtex/2cc1b9aa8927ac4952e93f34094a3eaaf/brefeld},
	interhash = {93d54534a08c30eda9e34d1def03ffa3},
	intrahash = {cc1b9aa8927ac4952e93f34094a3eaaf},
	journal = {Annals of Mathematical Statistics},
	keywords = {imported},
	pages = {400-407},
	timestamp = {2008-10-07T16:03:40.000+0200},
	title = {A stochastic approximation method},
	volume = 22,
	year = 1951
}












@inproceedings{hanzely2020stochastic,
	title={Stochastic subspace cubic {N}ewton method},
	author={Hanzely, Filip and Doikov, Nikita and Nesterov, Yurii and Richtarik, Peter},
	booktitle={International Conference on Machine Learning},
	pages={4027--4038},
	year={2020},
	organization={PMLR}
}



@InProceedings{Ghosh2020,
  author    = {Avishek Ghosh and Raj Kumar Maity and Arya Mazumdar and Kannan Ramchandran},
  booktitle = {IEEE International Symposium on Information Theory (ISIT)},
  title     = {Communication Efficient Distributed Approximate {N}ewton Method},
  year      = {2020},
  doi       = {10.1109/ISIT44484.2020.9174216},
}

@Article{DAN2020,
  author  = {Jiaqi Zhang and Keyou You and Tamer Ba\c{s}ar},
  journal = {arXiv preprint arXiv:2002.07378},
  title   = {Distributed Adaptive {N}ewton Methods with Globally Superlinear Convergence},
  year    = {2020},
}


@Article{NewtonByzantine2020,
  author  = {Avishek Ghosh and Raj Kumar Maity and Arya Mazumdar},
  journal = {arXiv preprint arXiv:2006.08737},
  title   = {Distributed {N}ewton Can Communicate Less and Resist Byzantine Workers},
  year    = {2020},
}



@InProceedings{DINGO2019,
  author    = {Crane, Rixon and Roosta, Fred},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {{DINGO}: Distributed {N}ewton-Type Method for Gradient-Norm Optimization},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {9498--9508},
  publisher = {Curran Associates, Inc.},
  volume    = {32},
  url       = {https://proceedings.neurips.cc/paper/2019/file/9718db12cae6be37f7349779007ee589-Paper.pdf},
}


%%%%%%%% 2018 %%%%%%%%

@inproceedings{GIANT2018,
 author = {Wang, Shusen and Roosta, Fred and Xu, Peng and Mahoney, Michael W},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {2332--2342},
 publisher = {Curran Associates, Inc.},
 title = {GIANT: Globally Improved Approximate Newton Method for Distributed Optimization},
 url = {https://proceedings.neurips.cc/paper/2018/file/dabd8d2ce74e782c65a973ef76fd540b-Paper.pdf},
 volume = {31},
 year = {2018}
}


@Article{PSN,
  author  = {Mutn\'{y}, Mojm\'{i}r and Richt\'{a}rik, Peter},
  journal = {Journal of Computational Mathematics},
  title   = {Parallel stochastic {N}ewton method},
  year    = {2018},
  number  = {3},
  pages   = {404--425},
  volume  = {36},
  groups  = {richtap:1},
}

@Article{KSJ-Newton2018,
  author  = {Karimireddy, Sai Praneeth and Stich, Sebastian U. and Jaggi, Martin},
  title   = {Global linear convergence of {N}ewton’s method without strong-convexity or {L}ipschitz gradients},
  journal = {arXiv:1806:0041},
  year    = {2018},
}


%%%%%%% 2017 %%%%%%%%%%%


@Article{NewtonSketch,
  author  = {Pilanci, Mert and Wainwright, Martin},
  title   = {{N}ewton Sketch: A Linear-time Optimization Algorithm with Linear-Quadratic Convergence},
  journal = {SIAM Journal on Optimization},
  year    = {2017},
  volume  = {27},
  number  = {1},
  pages   = {205--245},
  url     = {https://arxiv.org/pdf/1505.02250.pdf},
}

%%%%%%% 2016 %%%%%%%%%%%

@InProceedings{SDNA,
  author    = {Qu, Zheng and Richt\'{a}rik, Peter and Tak\'{a}\v{c}, Martin and Fercoq, Olivier},
  booktitle = {The 33rd International Conference on Machine Learning},
  title     = {{SDNA:} {S}tochastic dual {N}ewton ascent for empirical risk minimization},
  year      = {2016},
  pages     = {1823--1832},
  groups    = {richtap:1},
}


%%%%%%% 2015 %%%%%%%%%%%

@InProceedings{DISCO,
  author    = {Yuchen Zhang and Lin Xiao},
  title     = {{D}i{SCO}: Distributed Optimization for Self-Concordant Empirical Loss},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning, PMLR},
  year      = {2015},
  volume    = {37},
  pages     = {362--370},
}


@InProceedings{DANE,
  author    = {Ohad Shamir and Nati Srebro and Tong Zhang},
  title     = {Communication-Efficient Distributed Optimization using an Approximate {N}ewton-type Method},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning, PMLR},
  year      = {2014},
  volume    = {32},
  pages     = {1000--1008},
}






@Article{PN2006-cubic,
  author  = {Nesterov, Yurii and Polyak, Boris T.},
  title   = {Cubic regularization of {N}ewton method and its global performance},
  journal = {Mathematical Programming},
  year    = {2006},
  volume  = {108},
  number  = {1},
  pages   = {177--205},
}





@article{Schraudolph2006,
author = {Schraudolph, Nicol N and Simon, G},
file = {:home/gowerrobert/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schraudolph, Simon - 2006 - A Stochastic Quasi-Newton Method for Online Convex Optimization.pdf:pdf},
journal = {In Proceedings of 11th International Conference on Artificial Intelligence and Statistics},
title = {{A Stochastic Quasi-Newton Method for Online Convex Optimization}},
year = {2007}
}


@Article{RBFGS2020,
  author  = {Kovalev, Dmitry and  Gower, Robert M. and Richt\'{a}rik, Peter and Rogozin,  Alexander},
  journal = {arXiv:2002.11337},
  title   = {Fast Linear Convergence of Randomized {BFGS}},
  year    = {2020},
}

@InProceedings{SN2019,
  author      = {Kovalev, Dmitry and Mishchenko, Konstanting and Richt\'{a}rik, Peter},
  title       = {Stochastic {N}ewton and Cubic {N}ewton Methods with Simple Local Linear-Quadratic Rates},
  booktitle   = {NeurIPS Beyond First Order Methods Workshop},
  year        = {2019},
  institution = {KAUST},
}





@TechReport{Griewank-cubic-1981,
  author      = {Griewank, Andreas},
  title       = {The modification of {N}ewton’s method for unconstrained optimization by bounding cubic terms},
  institution = {Department of Applied Mathematics and Theoretical Physics, University of Cambridge},
  year        = {1981},
  note        = {Technical Report NA/12},
}


@InProceedings{RBCN,
  author    = {Doikov, Nikita and Richt\'{a}rik, Peter},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  title     = {Randomized Block Cubic {N}ewton Method},
  year      = {2018},
  address   = {Stockholmsmässan, Stockholm Sweden},
  editor    = {Dy, Jennifer and Krause, Andreas},
  month     = {10--15 Jul},
  pages     = {1290--1298},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  abstract  = {We study the problem of minimizing the sum of three convex functions: a differentiable, twice-differentiable and a non-smooth term in a high dimensional setting. To this effect we propose and analyze a randomized block cubic Newton (RBCN) method, which in each iteration builds a model of the objective function formed as the sum of the natural models of its three components: a linear model with a quadratic regularizer for the differentiable term, a quadratic model with a cubic regularizer for the twice differentiable term, and perfect (proximal) model for the nonsmooth term. Our method in each iteration minimizes the model over a random subset of blocks of the search variable. RBCN is the first algorithm with these properties, generalizing several existing methods, matching the best known bounds in all special cases. We establish ${\cal O}(1/\epsilon)$, ${\cal O}(1/\sqrt{\epsilon})$ and ${\cal O}(\log (1/\epsilon))$ rates under different assumptions on the component functions. Lastly, we show numerically that our method outperforms the state-of-the-art on a variety of machine learning problems, including cubically regularized least-squares, logistic regression with constraints, and Poisson regression.},
  file      = {doikov18a.pdf:http\://proceedings.mlr.press/v80/doikov18a/doikov18a.pdf:PDF},
  groups    = {richtap:1},
  url       = {http://proceedings.mlr.press/v80/doikov18a.html},
}


@InCollection{RSN,
  author    = {Gower, Robert Ma and Kovalev, Dmitry and Lieder, Felix and Richt\'{a}rik, Peter},
  title     = {{RSN}: {R}andomized {S}ubspace {N}ewton},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d'Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {616--625},
  url       = {http://papers.nips.cc/paper/8351-rsn-randomized-subspace-newton.pdf},
}

@article{Byrd2011,
author = {Byrd, Richard H and Chin, Gillian M and Neveitt, Will and Nocedal, Jorge},
file = {:home/gowerrobert/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Byrd et al. - 2011 - On the use of stochastic hessian information in optimization methods for machine learning(4).pdf:pdf},
journal = {SIAM Journal on Optimization},
keywords = {1,10,10079923x,1137,65k05,90c06,90c30,90c55,ams subject classifications,and the limited memory,bfgs,doi,introduction,machine learning,stochastic optimization,the inexact newton-cg method,unconstrained optimization},
number = {3},
pages = {977--995},
title = {{On the Use of Stochastic Hessian Information in Optimization Methods for Machine Learning}},
volume = {21},
year = {2011}
}


@InProceedings{Mokhtari-Ling-Ribeiro-2015,
  author    = {A. Mokhtari and Q. Ling and A. Ribeiro},
  booktitle = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {An approximate {N}ewton method for distributed optimization},
  year      = {2015},
  pages     = {2959-2963},
  doi       = {10.1109/ICASSP.2015.7178513},
}



@InProceedings{Jadbabaie2009,
  author    = {A. Jadbabaie and A. Ozdaglar and M. Zargham},
  booktitle = {Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference},
  title     = {A distributed {N}ewton method for network optimization},
  year      = {2009},
  pages     = {2736-2741},
  doi       = {10.1109/CDC.2009.5400289},
}



@Article{AIDE,
  author  = {Reddi, Sashank J. and Kone\v{c}n\'{y}, Jakub and Richt\'{a}rik, Peter and P\'{o}czos, Barnab\'{a}s and Smola, Alex},
  journal = {arXiv:1608.06879},
  title   = {{AIDE}: fast and communication efficient distributed optimization},
  year    = {2016},
  groups  = {richtap:1},
}









@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

\documentclass[12pt]{article}



%\usepackage{natbib}
\usepackage[sort, numbers]{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[english, russian]{babel}
\newcommand{\squeeze}{}
%%%%%%%

%%% for coloring rows in a table
%\usepackage[flushleft]{threeparttable}
\usepackage{threeparttable}


\usepackage{multirow}
\usepackage{colortbl}
\definecolor{bgcolor}{rgb}{0.8,1,1}
\definecolor{bgcolor2}{rgb}{0.8,1,0.8}

%%%%%%%%
\usepackage{graphicx} %Loading the package
\graphicspath{{../Experiments/}}

\newcommand{\eqdef}{\; { := }\;}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Exp}{\mathbb{E}}
\newcommand{\ExpBr}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\norm}[1]{\left\|#1\right\|}
\def\<#1,#2>{\langle #1,#2\rangle}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\prox}{\operatorname{prox}}
\newcommand{\footeq}[1]{{\footnotesize #1}}

\newcommand{\newalpha}{h}

\usepackage{tcolorbox}
\usepackage{pifont}
\definecolor{mydarkgreen}{RGB}{39,130,67}
\definecolor{mydarkred}{RGB}{192,47,25}
\newcommand{\green}{\color{mydarkgreen}}
\newcommand{\red}{\color{mydarkred}}
\newcommand{\cmark}{\green\ding{51}}%
\newcommand{\xmark}{\red\ding{55}}%

\newcommand{\mA}{\mathbf{A}}
\newcommand{\mB}{\mathbf{B}}
\newcommand{\mC}{\mathbf{C}}
\newcommand{\mH}{\mathbf{H}}
\newcommand{\mI}{\mathbf{I}}
\newcommand{\mU}{\mathbf{U}}
\newcommand{\mZ}{\mathbf{Z}}

\newcommand{\cC}{{\mathcal{C}}}
\newcommand{\cH}{{\mathcal{H}}}

\usepackage{amsmath,amsfonts,amssymb,amsthm,array}


\usepackage{mdframed} 
\usepackage{thmtools}
\usepackage{textcomp}

%\theoremstyle{shaded}
\declaretheorem[within=section]{definition}
\declaretheorem[sibling=definition]{theorem}
\declaretheorem[sibling=definition]{proposition}
\declaretheorem[sibling=definition]{assumption}
\declaretheorem[sibling=definition]{corollary}
\declaretheorem[sibling=definition]{conjecture}
\declaretheorem[sibling=definition]{lemma}
\declaretheorem[sibling=definition]{example}
\declaretheorem[sibling=definition]{remark}


% TO DO NOTES 
\usepackage[colorinlistoftodos,bordercolor=orange,backgroundcolor=orange!20,linecolor=orange,textsize=scriptsize]{todonotes}


\usepackage{microtype}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables


\usepackage{grffile}


\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\textheight=24cm % высота текста
\textwidth=16cm % ширина текста
\oddsidemargin=0pt % отступ от левого края
\topmargin=-1.5cm % отступ от верхнего края
\parindent=24pt % абзацный отступ
\parskip=0pt % интервал между абзацами
\tolerance=2000 % терпимость к "жидким" строкам
\flushbottom % выравнивание высоты страниц

%\graphicspath{ {fig/} }



\begin{document}

\thispagestyle{empty}
\begin{center}
    \sc
            «Московский физико-технический институт 
        {\rm (национальный
исследовательский университет)}\\
        Физтех-школа прикладной математики и информатики\\
        Кафедра <<Интеллектуальные системы>>\\[35mm]
    \rm\large
        Исламов Рустем Ильфакович\\[10mm]
    \bf\Large
		Распределенные методы второго порядка с быстрой скоростью сходимости и компрессией\\[10mm]
    \rm\normalsize
        03.03.01 ~--- Прикладные математика и физика\\[10mm]
    \sc
        Выпускная квалификационная работа Бакалавра\\[10mm]
\end{center}
\hfill\parbox{80mm}{
    \begin{flushleft}
    \bf
        Научный руководитель:\\
    \rm
        д.~ф.-м.~н. Стрижов Вадим Викторович\\[5cm]
    \end{flushleft}
}
\begin{center}
    Москва\\
    2021
\end{center}

\clearpage
\begin{abstract}

	Данная бакалаврская диссертация основана на статье <<Distributed Second Order Methods with Fast Rates
and Compressed Communication>> \citep{islamov2021distributed} за авторством Рустема Исламов, Шун
Кяна  и Питера Рихтарика.

  Мы разработали несколько новых эффективных с точки зрения коммуникации методов второго порядка для распределенной оптимизации. Наш первый метод, NEWTON-STAR, является одним из вариантов метода Ньютона, от которого он наследует свою быструю локальную квадратичную скорость. Однако, в отличие от метода Ньютона, NEWTON-STAR имеет ту же стоимость коммуницирования за одну итерацию, что и градиентный спуск. Хотя этот метод непрактичен, поскольку он опирается на использование некоторых неизвестных параметров, характеризующих Гессиан целевой функции в оптимуме, он служит отправной точкой, которая позволяет нам проектировать практические варианты с доказанными теоретическими гарантиями сходимости. В частности, мы разработали стратегию, основанную на использовании стохастической разреженности для изучения неизвестных параметров итеративным способом, сохраняя эффективность с точки зрения коммуникаций. Применение этой стратегии к NEWTON-STAR приводит к нашему следующему методу, NEWTON-LEARN, для которого мы доказали локальные линейные и сверхлинейные скорости сходимости, не зависящие от числа обусловленности функции. Когда эти методы применимы, они имеют значительно более высокие скорости сходимости по сравнению с современными методами. Наши результаты подтверждаются экспериментальными результатами на реальных наборах данных и показывают улучшение на несколько порядков по сравнению с базовыми и современными методами с точки зрения эффективности коммуницирования.
\end{abstract}

\clearpage
\selectlanguage{english} 
\begin{abstract}
	This bachelor thesis is based on paper ``Distributed Second Order Methods with Fast Rates
and Compressed Communication'' \citep{islamov2021distributed} written by Rustem Islamov, Xun Qian, and Peter Richtárik.	
	
	We develop new communication-efficient second-order method for distributed optimization. Our first method, {\sf NEWTON-STAR}, is a variant of Newton's method from which it inherits its fast local quadratic rate. However, unlike Newton's method, {\sf NEWTON-STAR} enjoys the same per iteration communication cost as gradient descent. While this method is impractical as it relies on the use of certain unknown parameters characterizing the Hessian of the objective function at the optimum,  it serves as the starting point which enables us design practical variants thereof with strong theoretical guarantees. In particular, we design a stochastic sparsification strategy for learning the unknown parameters in an iterative fashion in a communication efficient manner. Applying this strategy to {\sf NEWTON-STAR} leads to our next method, {\sf NEWTON-LEARN}, for which we prove  local linear and superlinear rates independent of the condition number. When applicable, this method can have dramatically superior convergence behavior when compared to state-of-the-art methods. Our results are supported with experimental results on real datasets, and show several orders of magnitude improvement on baseline and state-of-the-art methods in terms of communication complexity. 
\end{abstract}

\clearpage
{
%\footnotesize
\tableofcontents
}


\clearpage
\section{Introduction}
	The prevalent paradigm for training modern supervised machine learning models is based on (regularized) empirical risk minimization (ERM) \citep{shai_book}, and the most commonly used optimization methods deployed for solving ERM problems belong to the class of  stochastic first order methods \citep{RobbinsMonro:1951,  Nemirovski-Juditsky-Lan-Shapiro-2009}. Since modern training data sets are very large and are becoming larger every year, it is increasingly harder to get by without relying on modern computing architectures which make efficient use of  distributed computing.  However, in order to develop efficient distributed methods, one has to keep in mind that communication among the different parallel workers (e.g. processors or compute nodes) is typically very slow, and almost invariably forms the main bottleneck in deployed optimization software and systems \citep{bekkerman2011scaling}. For this reason, further advances in the area of communication efficient distributed first order optimization methods for solving ERM problems are highly needed, and research in this area constitutes one  of the most important fundamental endeavors in modern machine learning. Indeed, this research  field is very active, and numerous advances  have been made over the past decade \citep{Seide14, Wen17, Alistarh17, Bernstein18, DIANA, Stich19, Tang19}.
 
  \subsection{Distributed optimization}
 
We consider L2 regularized empirical risk minimization problems of the form
\begin{equation}\label{primal}
\squeeze 
\min \limits_{x\in \mathbb{R}^d} \left[ P(x) \eqdef f(x) + \frac{\lambda}{2}\|x\|^2  \right],
\end{equation}
where $f:\R^d \to \R$ is a smooth\footnote{Function $\phi:\R^d\to \R$ is {\em smooth} if it is differentiable, and has $L_\phi$ Lipschitz gradient: $\|\nabla \phi(x)- \nabla \phi(y)\| \leq L_\phi  \|x-y\|$ for all $x,y\in \R^d$. We say that $L_\phi$ is the {\em smoothness constant} of $\phi$.} convex function of the ``average of averages'' structure
\begin{equation} \label{eq:f_and_f_i} 
 \squeeze f(x) \eqdef\frac{1}{n}\sum \limits_{i=1}^n  f_i(x), \quad f_i(x) \eqdef \frac{1}{m} \sum \limits \limits_{j=1}^{m} f_{ij}(x), \end{equation}
and $\lambda\geq 0$ is a regularization parameter. Here $n$ is the number of parallel workers (nodes), and $m$ is the number of training examples handled by each node\footnote{All our results can be extended in a straightforward way to the more general case when node $i$ contains $m_i$ training examples. We decided to present the results in the special case $m=m_i$ for all $i$ in order to simplify the notation. }.  
The value $f_{ij}(x)$ denotes the loss of the model parameterized by vector $x\in \R^d$ on the $j^{\rm th}$ example owned by the $i^{\rm th}$ node. This example is denoted as $a_{ij} \in \R^d$, and the corresponding loss function is $\varphi_{ij}:\R \to \R$, and hence we have
\begin{equation}\label{eq:f_ij} f_{ij}(x) \eqdef \varphi_{ij}(a_{ij}^\top x).\end{equation}

Thus, $f$ represents the average loss/risk over all $nm$ training datapoints, and problem \eqref{primal} seeks to find the model whose (L2 regularized) empirical risk is minimized. We make the following assumption throughout the paper. 
\begin{assumption}\label{as:general}
Problem (\ref{primal}) has at least one optimal solution $x^*$. For all  $i$ and $j$, the loss function $\varphi_{ij}: \mathbb{R} \to \mathbb{R}$ is $\gamma$-smooth, twice differentiable, and its second derivative $\varphi_{ij}^{\prime\prime} : \R \to \R$ is $\nu$-Lipschitz continuous.
\end{assumption} 

Note that in view of \eqref{eq:f_ij}, the Hessian of $f_{ij}$ at point $x$ is
\begin{equation}
\label{eq:87ybfd0fd}
\mH_{ij}(x) \eqdef 
\nabla^2 f_{ij}(x) = 
\newalpha_{ij}(x) a_{ij}a_{ij}^\top, 
\end{equation}
where
\begin{equation}\label{eq:h_ij-def} \newalpha_{ij}(x) \eqdef \varphi^{\prime\prime}_{ij}(a_{ij}^\top x).\end{equation} In view of Assumption~\ref{as:general}, we have $|\varphi''_{ij}(t)| \leq \gamma$ for all $t\in \R$, and 
\begin{equation}
|\newalpha_{ij}(x) - \newalpha_{ij}(y)| \leq \nu |a_{ij}^\top x - a_{ij}^\top y| \leq \nu \|a_{ij}\|  \|x-y\| \label{eq:alphaijL} 
\end{equation}
for all $x, y \in \R^d$. Let $R\eqdef \max_{ij} \|a_{ij}\|$. The Hessian of $f_i$ is given by
\begin{equation}\label{eq:H_i} \squeeze \mH_i(x) \overset{\eqref{eq:f_and_f_i}  }{=}
%\nabla^2 f_i(x)  
\frac{1}{m}\sum\limits_{j=1}^m \mH_{ij}(x) \overset{\eqref{eq:87ybfd0fd}}{=} \frac{1}{m}\sum\limits\limits_{j=1}^m \newalpha_{ij}(x) a_{ij}a_{ij}^\top,\end{equation}
and the Hessian of $f$ is given by
\begin{equation}\label{eq:H}\squeeze \mH(x) \overset{\eqref{eq:f_and_f_i}  }{=}
%\nabla^2 f_i(x)  
\frac{1}{n}\sum\limits_{i=1}^n \mH_{i}(x) \overset{\eqref{eq:H_i}}{=} \frac{1}{nm}\sum \limits_{i=1}^n   \sum\limits_{j=1}^m \newalpha_{ij}(x) a_{ij}a_{ij}^\top.\end{equation}


\subsection{The curse of the condition number}
 
All first order methods~---distributed or not~---suffer from a dependence on an appropriately chosen notion of a {\em condition number\footnote{Example: if one wishes to minimize an $L$-smooth $\mu$-strongly convex function  and one cares about the number of gradient type iterations, the appropriate notion of a condition number is $\kappa \eqdef \frac{L}{\mu}$.}}~---a number that describes the difficulty of solving the problem by the method at hand. A condition number is a function of the goal we are trying to achieve (e.g., minimize the number of iterations vs minimize the  number of communications), choice of the loss function, structure of the model we are trying to learn, and last but not least, the size and properties of the training data. In fact, most research in this area is motivated by the desire to design methods that would have a {\em reduced} dependence on the condition number. This is the case for many of the tricks heavily studied in the literature, including minibatching~\citep{pegasos2}, importance sampling~\citep{NeedellWard2015, IProx-SDCA}, random reshuffling~\citep{RR}, variance reduction~\citep{schmidt2017minimizing, johnson2013accelerating, proxSVRG, SAGA}, momentum~\citep{SHB-NIPS, SMOMENTUM}, adaptivity \citep{MM2019}, communication compression~\citep{Alistarh17, Bernstein18, DIANA}, and local computation~\citep{COCOA+journal, localSGD-Stich,localSGD-AISTATS2020}.  Research in this area is becoming saturated, and new ideas are needed to make further progress.

\subsection{Newton's method to the rescue?} One of the ideas that undoubtedly crossed everybody's mind is the trivial observation that there {\em is} a very old and simple method which does {\em not} suffer from any conditioning issues: Newton's method. Indeed, when it works, Newton's method has a fast {\em local quadratic convergence rate} which is entirely independent of the condition number of the problem \citep{Beck-book-nonlinear}. While this is a very attractive property, developing  scalable distributed variants of Newton's method that could also {\em provably outperform} gradient based methods remains a largely unsolved problem. To highlight the severity of the issues with extending Newton's method to stochastic and distributed settings common in machine learning, we note that until recently, we did not even have any Newton-type analogue of SGD that could provably work with small minibatch sizes, let alone minibatch size one \citep{SN2019}. In contrast, SGD with minibatch size one is one of the simplest and well understood variants thereof \citep{NeedellWard2015}, and much of modern development in the area of SGD methods is much more sophisticated.  Most variants of Newton's method proposed for deployment in machine learning are heuristics, which is to say that they are not supported with any convergence guarantees, or have convergence guarantees without explicit rates, or suffer from rates that are worse than the rates of first order methods.  


\subsection{Contributions summary}

We develop {\em several  new  fundamental Newton-type methods} which we hope make a marked step towards the ultimate goal of developing practically useful and {\em communication efficient distributed second order methods}. Our methods are designed with the explicit goal of supporting {\em efficient communication} in a distributed setting, and in sharp contrast with most recent work, their design was heavily influenced by our desire to equip them with {\em strong convergence guarantees} typical for the classical Newton's method \citep{Wallis1685, Raphson1697} and  cubically regularized Newton's method~\citep{Griewank-cubic-1981, PN2006-cubic}. Our convergence results are summarized in Table~\ref{tbl:rates}.

\begin{table}[t]
	\caption{Summary of algorithms proposed and convergence results proved in this paper.}
	\label{tbl:rates}
	\begin{center}
		  \begin{threeparttable}
		\scriptsize
			\begin{tabular}{|c | ccc | cc|}
				\hline
				                     & \multicolumn{3}{c|}{\bf Convergence} &  &   \\
				  {\bf Method} &  {\bf result} ${}^\dagger$   &  {\bf type}  & {\bf rate}  & \begin{tabular}{c}{ \bf Rate} \\ {\bf  of the}\\ {\bf condition number?}  \end{tabular} & {\bf Theorem }\\
				\hline
				\begin{tabular}{c} {\sf NEWTON-STAR} \\ \eqref{eq:Newton-star} \end{tabular} & $r_{k+1}\leq c r_k^2$   & local & quadratic & \cmark &  \ref{th:localquadratic}\\ \hline	
%%%%%%%%%%%%%%%%%%%%			 				
			 \multirow{2}{*}{ \begin{tabular}{c} {\sf NEWTON-LEARN}  \\ Algorithm~\ref{alg:NL1} \end{tabular}  } 
			 &  $\Phi^k_1 \leq \theta_1^k \Phi_1^0$ & local & linear & \cmark &  \ref{th:lambda>0}\\ 
			 &  $r_{k+1} \leq c \theta_1^k  r_k$ & local & superlinear & \cmark &  \ref{th:lambda>0}\\ \hline	
%%%%%%%%%%%%%%%%%%%%					 																
          \hline
			\end{tabular}
     \end{threeparttable}			
    \begin{tablenotes}
      {\scriptsize      
  \item   Quantities for which we prove convergence:  (i) distance to solution $r_k \eqdef \norm{x^k-x^*}$; (ii) Lyapunov function $\Phi^k_q \eqdef \norm{x^k-x^*}^2 + c_q \sum_{i=1}^n \sum_{j=1}^m ( h_{ij}^k - h_{ij}(x^*) )^2$ for $q=1,2,3$, where $h_{ij}(x^*) = \varphi''_{ij}(a_{ij}^\top x^*)$ (see \eqref{eq:h_ij-def}); (iii) Function value suboptimality  $\Delta_k \eqdef P(x^k) - P(x^*)$
        \item ${}^\dagger$ constant $c$ is possibly different each time it appears in this table. Refer to the precise statements of the theorems for the exact values.
        }
    \end{tablenotes}			
	\end{center}
\end{table}


\begin{itemize}
\item {\bf First new method and its local quadratic convergence.} We first show that if we know the Hessian of the objective function at   the optimal solution, then we can use it instead of the typical Hessian appearing in Newton's method, and the resulting algorithm, which we call {\sf NEWTON-STAR (NS)}, inherits  local quadratic convergence behavior of Newton's method (see Theorem~\ref{th:localquadratic}). In a distributed setting with a central orchestrating sever, each compute node only needs to send the local  gradient  to the server  node, and no matrices need to be sent. While this method is not practically useful, it acts as a stepping stone to our next method, in which these deficiencies are removed. This method is described in Section~\ref{sec:3steps}. 

\item  {\bf Second new method and its local linear and superlinear convergence.} Motivated by the above result, we propose a {\em learning scheme which enables us to learn the Hessian at the optimum iteratively  in a communication efficient manner.} This scheme gives rise to our second new method: {\sf NEWTON-LEARN (NL)}. We analyze this method in the case, when all individual loss functions are convex and $\lambda >0$. Besides the local full gradient, each worker node needs to send additional information to the server node in order to learn the Hessian at the optimum. However, our learning  scheme supports {\em compressed communication} with arbitrary compression level. This level can be chosen so that in each iteration, each node sends an equivalent of a few gradients to the server only. That is, we can achieve $O(d)$ communication complexity in each iteration. We prove local linear convergence for a carefully designed Lyapunov function, and  local superlinear convergence for the squared distance to optimum (see Theorems~\ref{th:lambda>0}). Remarkably, all these rates are {\em independent of the condition number.} The {\sf NL} method and the associated theory are described in Section~\ref{sec:Newton-learn}.


\item {\bf Experiments.} Our theory is corroborated with numerical experiments showing the superiority of our method to several state-of-the-art benchmarks, including DCGD~\citep{KFJ}, DIANA \citep{DIANA, DIANA2}, ADIANA \citep{ADIANA}, BFGS~\cite{Broyden1967, Fletcher1970, Goldfarb1970, shanno1970conditioning}, and DINGO \citep{DINGO2019}.  Our method can achieve communication complexity which is {\em several orders of magnitude better} than  competing methods (see Section~\ref{sec:experiments}).
\end{itemize}






\subsection{Related work}

Several distributed Newton-type methods can be found in recent literature. DANE \citep{DANE} is a distributed approximate Newton-type method where each worker node needs to solve a subproblem using the full gradient at each iteration, and the new iterate is the average of these subproblem solutions. The linear convergence of DANE was obtained in the strongly convex case. An inexact DANE method in which the subproblem is solved approximately was proposed and studied by \citet{AIDE}. Moreover, an accelerated version of inexact DANE, called AIDE, was proposed in  \citep{AIDE} by a generic acceleration scheme~---catalyst~\citep{lin2015universal}~---and an optimal communication complexity can be obtained up to logarithmic factors in specific settings. The DiSCO method, which combines inexact damped Newton method and distributed preconditioned conjugate gradient method, was proposed by \citet{DISCO} and analyzed for self-concordant empirical loss. GIANT \citep{GIANT2018} is a globally improved approximate Newton method which has a better linear convergence rate than first-order methods for quadratic functions, and has local linear-quadratic convergence for strongly convex functions. GIANT and DANE are identical for quadratic programming. The communication cost per iteration of the above methods is $O(d)$. These methods can only achieve linear convergence in the strongly convex case. The comparison of the iteration complexity of the above methods for the ridge regression problem can be found in Table 2 of \citep{GIANT2018}. 




\begin{table}[t]
	\caption{Comparison of distributed Newton-type methods. Our methods combine the best of both worlds, and are the only methods we know about which do so: we obtain fast rates independent of the condition number, and allow for $O(d)$ communication per communication round.}
	\label{tablecomparison}
	\begin{center}
		\scriptsize
			  \begin{threeparttable}
			\begin{tabular}{cccccc}
				\toprule
				 {\bf Method}  &  \begin{tabular}{c} {\bf Convergence} \\ {\bf rate} \end{tabular} & \begin{tabular}{c}{ \bf Rate} \\ {\bf independent of the}\\ {\bf condition number?}  \end{tabular} &\begin{tabular}{c}{\bf Communication} \\ {\bf  cost}\\ {\bf per iteration}  \end{tabular}  & \begin{tabular}{c}  {\bf Network} \\ {\bf  structure}  \end{tabular} \\
				\midrule 
				\begin{tabular}{c} DANE \\ \citep{DANE} \end{tabular}   & Linear & \xmark & $O(d)$ & Centralized \\ \hline
				\begin{tabular}{c} DiSCO \\ \citep{DISCO}  \end{tabular}   & Linear &  \xmark & $O(d)$ & Centralized \\ \hline 
				\begin{tabular}{c} AIDE \\ \citep{AIDE}  \end{tabular}   & Linear &  \xmark & $O(d)$ & Centralized \\ \hline 
				\begin{tabular}{c} GIANT \\ \citep{GIANT2018}  \end{tabular}   & Linear &  \xmark & $O(d)$ & Centralized \\ \hline
				\begin{tabular}{c} DINGO \\ \citep{DINGO2019}   \end{tabular}   & Linear &  \xmark & $O(d)$ & Centralized \\ \hline 
				\begin{tabular}{c} DAN \\ \citep{DAN2020}   \end{tabular}   & Local quadratic${}^\dagger$ & \cmark & $O(nd^2)$ & Decentralized \\ \hline 
				\begin{tabular}{c} DAN-LA \\ \citep{DAN2020}   \end{tabular}   & Superlinear & \cmark & $O(nd)$ & Decentralized \\ \hline 
				\begin{tabular}{c} {\sf NEWTON-STAR} \\ {\bf this work}  \end{tabular}   &Local  quadratic & \cmark & $O(d)$ & Centralized \\ \hline 
				\begin{tabular}{c} {\sf NEWTON-LEARN} \\ {\bf this work}  \end{tabular}   &Local  superlinear & \cmark & $O(d)$ & Centralized \\ \hline 
				\bottomrule
			\end{tabular}
			\end{threeparttable}
    \begin{tablenotes}
      {\scriptsize      
  \item   ${}^\dagger$ DAN converges globally, but the quadratic rate is introduced only after $O(L_2/\mu^2)$ steps, where $L_2$ is the Lipschitz constant of the Hessian of $P$, and $\mu$ is the strong convexity parameter of $P$. This is a property it inherits from the recent method of Polyak \citep{L-Newton2019} this method is based on.
        }
    \end{tablenotes}				
	\end{center}
\end{table}

\citet{DINGO2019} proposed a distributed Newton-type method called DINGO   for solving  invex finite-sum problems. Invexity is a special case of non-convexity, which subsumes convexity as a sub-class. A linear convergence rate was obtained for DINGO under certain assumptions using an Armijo-type line search, and at each iteration, several communication rounds are needed assuming two communication rounds for line-search per iteration. The communication cost for each communication round is $O(d)$. The compressed version of DINGO was studied in \citep{Ghosh2020} to reduce the communication cost at each communication round by using the $\delta$-approximate compressor, and the same rate of convergence as DINGO can be obtained by properly choosing the stepsize and hyper-parameters when $\delta$ is a constant.  \citet{DAN2020} proposed two decentralized distributed adaptive Newton methods, called DAN and DAN-LA. DAN combines the distributed selective flooding (DSF) algorithm and Polyak’s adaptive Newton method \citep{polyak2020new}, and enters pure Newton method which has quadratic convergence after about $\frac{2M}{\mu^2} \|\nabla P(x^0)\|$ iterations, where $M$ is the Lipschitz constant of the Hessian of $P$ and $\mu$ is the strongly convex parameter of $P$. DAN-LA, which leverages the low-rank approximation method to reduce the communication cost, has global superlinear convergence. At each iteration, both DAN and DAN-LA need $n-1$ communication rounds, and the communication cost for each communication round is $O(d^2)$ and $O(d)$ respectively. 

We compare the convergence rate and per-iteration communication cost with these Newton-type methods in Table~\ref{tablecomparison}. Note that the first five methods in the table have rates that depend on the condition number of the problem, and as such, do not have the benefits normally attributed to pure Newton's method. Note also that the two prior methods which do have rates independent of the condition number have high cost of communication. Our methods combine the best of both worlds, and are the only methods we know about which do so: we obtain fast rates independent of the condition number, and allow for $O(d)$ communication per communication round.  We were able to achieve this by a complete redesign of how second order methods should work in the distributed setting. Our methods are not simple extensions of existing schemes, and our proofs use novel arguments and techniques.












\section{Three Steps Towards an Efficient Distributed Newton Type Method} \label{sec:3steps}

In order to better explain the  algorithms and results of this paper, we will proceed through several steps in a gradual explanation of the ideas that ultimately lead to our methods. While this is not the process we used to come up with our methods, in retrospect we believe that our methods and results will be understood more easily when seen as having been arrived at in this way. In other words, we have constructed what we believe is a plausible discovery story, one enabling faster and better comprehension. If these ideas seem to follow naturally, it is because we made a conscious effort to make then appear that way. The goal of this paper is to develop communication efficient variants of Newton's method for solving the distributed optimization problem \eqref{primal}.   

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Naive distributed implementation of Newton's method} \label{subsec:Newton}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


 Newton's method applied to problem \eqref{primal}  performs the iteration
 
 \begin{equation} \label{eq:Newton} x^{k+1} = x^k - \left(\nabla^2 P(x^k)\right)^{-1} \nabla P(x^k) \overset{\eqref{primal}}{=} x^k - \left(\mH(x^k) +\lambda \mI \right)^{-1} \nabla P(x^k).\end{equation}
 
A naive way to implement this method in the {\em parameter server} framework is for each node $i$ to compute the Hessian $\mH_i(x^k)$ and gradient $\nabla f_i(x^k)$ and to communicate these objects to the server. The server then averages the local Hessians $\mH_i(x^k)$ to produce $\mH(x^k)$ via \eqref{eq:H}, and averages the local gradients $\nabla f_i(x^k)$ to produce $\nabla f(x^k)$. The server then adds $\lambda \mI$ to the Hessian, producing $\mH(x^k) + \lambda \mI = \nabla^2 P(x^k)$, adds  $\lambda x^k$ to the gradient, producing $\nabla P(x^k) = \nabla f(x^k) + \lambda x^k$, and subsequently performs the Newton step \eqref{eq:Newton}. The resulting vector $x^{k+1}$ is then broadcasted to the nodes and the process is repeated. 

This implementation mirrors the way GD and many other first order methods are implemented in the parameter server framework. However, unlike in the case of GD, where only $O(d)$ floats need to be sent and received by each node in each iteration, the upstream communication in Newton's method requires $O(d^2)$ floats to be communicated by each worker to the server. Since $d$ is typically very large, this is prohibitive in practice. Moreover, computation of the Newton's step by the parameter server is much more expensive than simple averaging of the gradients performed by gradient type methods. However, in this paper we will not be concerned with the cost of the Newton step itself, as we will assume the server is powerful enough and the network connection is slow enough for this step not to be the main bottleneck of the iteration. Instead, we assume that the communication steps in general, and the $O(d^2)$ communication of the Hessian matrices in particular,  is what forms the bottleneck. The $O(d)$ per node communication cost of the local gradients is negligible,  and so is the $O(d)$ broadcast of the updated model.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A better implementation taking advantage of the structure of $\mH_{ij}(x)$} \label{subsec:Naive2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The above naive implementation can be improved in the setting when $m < d^2$ by taking advantage of the explicit structure \eqref{eq:H_i} of the local Hessians as a conic combination of positive semidefinite rank one matrices:
\begin{equation}\label{eq:Newton2}
\squeeze\mH_i(x) = \frac{1}{m}\sum \limits_{j=1}^m \newalpha_{ij}(x) a_{ij}a_{ij}^\top.\end{equation}
Indeed, assuming that the server has direct access to all the training data vectors $a_{ij}\in \R^d$  (these vectors can be sent to the server at the start of the process), node $i$ can send the $m$ coefficients $\newalpha_{i1}(x), \dots, \newalpha_{im}(x)$ to the server instead, and the server is then able to reconstruct the Hessian matrix $\mH_i(x)$ from this information. This way, each node sends $O(m+d)$ floats to the server, which is a substantial improvement on the naive implementation in the regime when $m\ll d^2$.  However, when $m\gg d$, the upstream communication cost is still substantially larger than the $O(d)$ cost of GD.  If the server does not have enough memory to store all vectors $a_{ij}$, this procedure does not work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{{\sf NEWTON-STAR}: Newton's method with a single Hessian} \label{subsec:Newton-star}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now introduce a simple idea which, surprisingly, enables us to {\em remove the need to iteratively communicate any coefficients altogether.}  Assume, for the sake of argument, that we know the values  $\newalpha_{ij}(x^*)$ for all $i,j$. That is, assume the server has access to coefficients $\newalpha_{ij}(x^*)$ for all $i,j$, and that each node $i$ has access to coefficients $\newalpha_{ij}(x^*)$  for $j=1,\dots,m$, i.e., to the vector
\begin{equation}\label{eq:8f0d8hfd}\newalpha_{i}(x) \eqdef (\newalpha_{i1}(x),\dots, \newalpha_{im}(x)) \in \R^m\end{equation}
for $x=x^*$.  Next, consider the following new Newton-like method which we call {\sf NEWTON-STAR (NS)}, where the ``star'' points to the method's reliance on the knowledge of the optimal solution $x^*$:
\begin{eqnarray}  x^{k+1} &=& x^k - \left(\nabla^2 P(x^*) \right)^{-1} \nabla P(x^k)\notag\\ 
&\overset{\eqref{primal}}{=}& x^k - \left(\mH(x^*) +\lambda \mI \right)^{-1} \left(\frac{1}{n}\sum_{i=1}^n \nabla f_i(x^k) + \lambda x^k\right).\label{eq:Newton-star}\end{eqnarray}

Since the server knows $\mH(x^*)$, all that the nodes need to communicate are the local gradients $\nabla f_i(x^k)$, which costs $O(d)$ per node. The server then computes $x^{k+1}$, broadcasts it back to the nodes, and the process is repeated. This method has the same per-iteration $O(d)$ communication complexity as GD. However, as we show next, the number of iterations (which is the same as the number of communications) of {\sf NEWTON-STAR} does not depend on the condition number~--- a property it borrows from the classical Newton's method. The following theorem says that {\sf NEWTON-STAR} enjoys {\em local quadratic convergence}.

\begin{theorem}[Local quadratic convergence]\label{th:localquadratic}
Let  Assumption \ref{as:general} hold,  and assume that $\mH(x^*) \succeq \mu^* \mI$ for some $\mu^* \geq 0$ (for instance, this holds if $f$ is $\mu^*$-strongly convex) and that $\mu^*+\lambda >0$. Then for any starting point $x^0 \in \R^d$, the iterates of {\sf NEWTON-STAR} for solving problem \eqref{primal}   satisfy the following inequality:
\begin{equation}
\label{eq:NS-rate}
 \squeeze
 \|x^{k+1} - x^*\| \leq \frac{\nu }{2(\mu^*+\lambda)} \cdot \left( \frac{1}{nm} \sum \limits_{i=1}^n \sum \limits_{j=1}^{m} \|a_{ij}\|^3 \right) \cdot \|x^k-x^*\|^2. 
\end{equation}
\end{theorem}
\begin{proof}
{\footnotesize
By the first order optimality conditions, we have 
\begin{equation}\nabla f(x^*) + \lambda x^* = 0.\label{eq:FOC}\end{equation}
Let $\mH_* \eqdef \mH(x^*)$. Since $\mH_* \succeq \mu^* \mI$, we have $\mH_* +\lambda \mI \succeq (\mu^* +\lambda) \mI$, and hence \begin{equation}\label{eq:b98gdf_9898fd_93} \norm{\left(\mH_* + \lambda \mI\right)^{-1} } \leq \frac{1}{\mu^*+\lambda}.\end{equation} 
Using \eqref{eq:FOC} and \eqref{eq:b98gdf_9898fd_93}  and subsequently applying Jensen's inequality  to the function $x\mapsto \norm{x}$, we get 
\begin{eqnarray}
	\|x^{k+1} - x^*\| &=&\left \| x^k - x^*  - \left(\mH_* +\lambda \mI \right)^{-1} \nabla P(x^k)  \right \|  \notag \\
	&\overset{\eqref{eq:FOC}}{=}& \left \| \left(\mH_* +\lambda \mI \right)^{-1} \left[ \left(\mH_* + \lambda \mI \right)(x^k - x^*) - \left(\nabla f(x^k) -\nabla f(x^*)+ \lambda (x^k-x^*)\right)   \right] \right \|  \notag \\ 
	&\overset{\eqref{eq:b98gdf_9898fd_93}}{\leq} &\frac{1}{\mu^*+\lambda} \left \|  \left(\mH_* + \lambda \mI \right) (x^k - x^*) - \left( \nabla f(x^k) - \nabla f(x^*)   \right) - \lambda (x^k -x^*) \right \|  \notag \\ 
	&= &\frac{1}{\mu^*+\lambda} \left \|  \frac{1}{n} \sum_{i=1}^{n}  \mH_i(x^*) (x^k - x^*) -  \frac{1}{n}\sum_{i=1}^n \left( \nabla f_i(x^k) - \nabla f_i(x^*)   \right) \right \|  \notag \\ 
	&\leq & \frac{1}{n (\mu^*+\lambda)} \sum_{i=1}^{n}  \left \|  \mH_i(x^*) (x^k - x^*) -  \left( \nabla f_i(x^k) - \nabla f_i(x^*)   \right) \right \|  \notag \\ 	
	&\overset{\eqref{eq:H_i} }{=} &\frac{1}{n (\mu^*+\lambda)} \sum_{i=1}^{n}  \left \| \frac{1}{m} \sum_{j=1}^m \left[\newalpha_{ij}(x^*) a_{ij} a_{ij}^\top (x^k - x^*)- (\nabla f_{ij}(x^k) - \nabla f_{ij}(x^*))\right] \right \|  . \label{eq:rand-opur-9}
	\end{eqnarray}
	
We now use the fundamental theorem of calculus to express difference of gradients $\nabla f_{ij}(x^k) - \nabla f_{ij}(x^*)$ in an integral, obtaining
\begin{equation}\label{eq:difnablaf}
\nabla f_{ij}(x^k) - \nabla f_{ij}(x^*) = \int_{0}^1 \nabla^2 f_{ij}(x^* + \tau (x^k-x^*)) (x^k-x^*) d\tau. 
\end{equation}

Plugging this representation into	\eqref{eq:rand-opur-9} and noting that $ \nabla^2 f_{ij}(x)\equiv \mH_{ij}(x)$ (see \eqref{eq:87ybfd0fd}), we can continue:
\begin{eqnarray}	
	\|x^{k+1} - x^*\| 	& \overset{\eqref{eq:rand-opur-9}+\eqref{eq:difnablaf}}{ \leq} & \frac{1}{n (\mu^*+\lambda)} \sum_{i=1}^n \left\|   \frac{1}{m}  \sum_{j=1}^{m} \left(  \newalpha_{ij}(x^*) a_{ij}a_{ij}^\top (x^k - x^*)\right.\right.\notag\\
	&& \quad \left.\left.-  \int_{0}^1 \mH_{ij} (x^* + \tau(x^k - x^*)) (x^k - x^*)d\tau  \right)   \right\|  \notag \\ 
	&\overset{\eqref{eq:87ybfd0fd}}{=}& \frac{1}{n (\mu^*+\lambda)} \sum_{i=1}^n  \left\| \frac{1}{m} \sum_{j=1}^{m} \left(  \newalpha_{ij}(x^*) a_{ij}a_{ij}^\top (x^k - x^*)\right.\right.\notag\\
	&&\quad \left.\left.-  \int_{0}^1 \newalpha_{ij} (x^* + \tau(x^k - x^*))a_{ij}a_{ij}^\top (x^k - x^*)d\tau  \right)\right\| \notag  \\ 
	&=& \frac{1}{n (\mu^*+\lambda)} \sum_{i=1}^n  \left\| \frac{1}{m} \sum_{j=1}^{m}  a_{ij}a_{ij}^\top (x^k - x^*) \left( \newalpha_{ij}(x^*) - \int_{0}^1 \newalpha_{ij}(x^* + \tau(x^k - x^*))  d\tau \right) \right\|  \notag \\ 
	&\leq& \frac{ \|x^k - x^*\|}{ (\mu^*+\lambda)} \frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^{m} \|a_{ij}\|^2 \left|   \int_{0}^1  \newalpha_{ij}(x^*) - \newalpha_{ij}(x^* + \tau(x^k - x^*))  d\tau  \right|. \label{eq:b98f9d8gfd}
\end{eqnarray}
In the last step we have again used Jensen's inequality applied to the function $x\mapsto \norm{x}$, followed by  inequalities of the form $\norm{\mA_{ij} x t_{ij}} \leq \norm{\mA_{ij}} \norm{x} |t_{ij}|$ for $\mA_{ij} = a_{ij}a_{ij}^\top $, $x=x^k-x^*$ and $t_{ij}\in \R$.

From (\ref{eq:alphaijL})  we obtain
$
| \newalpha_{ij}(x^*) - \newalpha_{ij}(x^* + \tau(x^k - x^*))| \leq \nu \tau \|a_{ij}\| \cdot \|x^k - x^*\|, 
$
which implies that 
$$
\left|   \int_{0}^1  \newalpha_{ij}(x^*) - \newalpha_{ij}(x^* + \tau(x^k - x^*))  d\tau  \right| \leq \int_{0}^1 \nu \tau \|a_{ij}\| \cdot \|x^k - x^*\| d\tau = \frac{\nu \|a_{ij}\|}{2} \cdot \|x^k - x^*\|. 
$$

Plugging this into \eqref{eq:b98f9d8gfd}, we finally arrive at \eqref{eq:NS-rate}.
}
\end{proof}


Note that we do not need to assume $f$ to be convex or strongly convex. All we need to assume is positive definiteness of the Hessian at the optimum. This implies local strong convexity, and since our convergence result is local, that is all we need.

\section{{\sf NEWTON-LEARN}: Learning the Hessian and Local Convergence Theory} \label{sec:Newton-learn}

In Sections~\ref{subsec:Newton}, \ref{subsec:Naive2} and \ref{subsec:Newton-star} we have gone through three steps in our story,  with the first true innovation and contribution of this paper being the {\sf NEWTON-STAR} method and its rate. We have now sufficiently prepared the ground to motivate our first {\em key} contribution: the {\sf NEWTON-LEARN} method. We only outline the basic insights behind this method here; the details are included in Section~\ref{sec:Newton-learn}.  



\subsection{The main iteration}
In {\sf NEWTON-LEARN} we maintain a sequence of vectors \begin{equation}\label{eq:h_i^k}h_i^k=(h_{i1}^k,\dots, h_{im}^k) \in \R^m, \nonumber \end{equation} for all $i=1,\dots,n$ throughout the iterations $k\geq 0$  with the goal of {\em learning} the values $\newalpha_{ij}(x^*)$ for all $i,j$. That is, we construct the sequence with the explicit intention to enforce 
\begin{equation}\label{eq:learn} h_{ij}^k \to \newalpha_{ij}(x^*) \qquad \text{as} \qquad k\to \infty.\end{equation}

Using $h_{ij}^k \approx \newalpha_{ij}(x^*)$ we  estimate the Hessian $\mH(x^*)$ via
\begin{equation}\label{eq:Newton2-xx} 
\squeeze \mH(x^*) \approx \mH^k \eqdef  \frac{1}{nm}\sum \limits_{i=1}^n \sum \limits_{j=1}^m h^k_{ij}  a_{ij}a_{ij}^\top,\end{equation}
and then perform a similar iteration to \eqref{eq:Newton-star}:
\begin{equation} \label{eq:Newton-learn} x^{k+1} = x^k - \left(\mH^k + \lambda \mI\right)^{-1} \nabla P(x^k).\end{equation}

\subsection{Learning the coefficients: the idea}
To complete the description of the method, we need to explain how the vectors $h_i^{k+1}$ are updated. This is also the place where we can force the method to be communication efficient. Indeed, if we can design a rule that would enforce the update vectors $h_i^{k+1}-h_i^k$ to be {\em sparse}, say \begin{equation}\label{eq:sparse_update}\|h_i^{k+1}-h_i^k\|_0 \leq s \end{equation} for some $1 \leq s \leq m$ and all $i$ and $k$, then  the upstream communication by each node in each iteration would be of the order $O(s+d)$ only (provided the server has access to all vectors~$a_{ij}$)! That is, each node $i$ only needs to communicate $s$ entries of the update vector $h_i^{k+1}-h_i^k$ as the rest are equal to zero, and each node also needs to communicate the $d$ dimensional gradient $\nabla f_i(x^k)$.  Note that $O(s+d)$ can be interpreted as an {\em interpolation} of the $O(m+d)$   per-iteration communication complexity  of the structure-aware implementation of Newton's method from  Section~\ref{subsec:Naive2}, and of the  $O(d)$   per-iteration communication complexity  of {\sf NEWTON-STAR} described in   Section~\ref{subsec:Newton-star}. 

In the more realistic regime when the server does {\em not} have access to the data~$\{a_{ij}\}$, we ask each worker $i$ to additionally send the corresponding $s$ vectors~$a_{ij}$, which costs extra $O(s d)$ in communication per node. However, when $s=O(1)$, this is the same per-iteration communication effort as that of GD.

We develop the update rule defining the evolution of the vectors $h_1^k, \dots, h_n^k$. This rule (see \eqref{eq: big78fd_8h9fd}) applies to the $\lambda>0$ case and leads to {\sf NEWTON-LEARN} which we call {\sf NL} (see Algorithm~\ref{alg:NL1}). This rule and the method are described in Section~\ref{subsec:NL1}. 


\subsection{Outline of fast local convergence theory} We show in Theorem~\ref{th:lambda>0} that {\sf NEWTON-LEARN} enjoys a local linear rate wrt a certain Lyapunov function which  involves the term $\|x^k - x^*\|^2$ and also all terms of the form $\|h^k_i - \newalpha_i(x^*)\|^2$. This means that i) the main iteration \eqref{eq:Newton-learn} works, i.e., $x^k$ converges to $x^*$ at a local linear  rate, and that ii) the learning procedure works, and the desired convergence described in \eqref{eq:learn} occurs at a local linear rate. In addition, we also establish a local superlinear rate of $\|x^k - x^*\|^2$. Remarkably,  these rates are {\em independent of any condition number, which is  in sharp contrast with virtually all results on distributed Newton-type methods we are aware of.} 

Moreover, we wish to remark that second order methods are not typically analyzed using a Lyapunov style analysis. Indeed, we only know of a couple works that do so.  First, \citet{SN2019} develop stochastic Newton and cubic Newton methods of a different structure and scope from ours. They do not consider distributed optimization nor communication compression. Second, \citet{RBFGS2020} develop a stochastic BFGS method. Again, their method and scope is very different from ours. Hence, {\em our analysis may be of independent interest as it  adds to the arsenal of theoretical tools which could be used in a more precise analysis of other second order methods.}


\subsection{Compressed learning}

Instead of merely relying on sparse updates for the vectors $h_i^k$ (see \eqref{eq:sparse_update}), we provide a more general communication compression strategy which includes sparsification as a special case \citep{Alistarh17}. We do so via the use of a {\em random compression} operator.  We say that a randomized map $\cC:\R^m\to \R^m$ is  a {\em compression operator (compressor)} if  there exists a constant $\omega \geq 0$ such that the following relations hold for all $x\in \R^m$:
\begin{eqnarray} 
\mathbb{E}[\cC(x) ] &=& x \label{eq:unbiased} \\ 
\mathbb{E}\|\cC(x)\|^2 &\leq &(\omega + 1)\|x\|^2.\label{eq:omega-variance}
\end{eqnarray} 
The identity compressor $\cC(x)\equiv x$ satisfies these relations with $\omega=0$. The larger the variance parameter $\omega$ is allowed to be, the easier it can be to construct a compressor $\cC$ for which the value $\cC(x)$ can be encoded using a small number of bits only. We refer the reader to \citep{biased2020} for a list of several compressors and their properties.


\subsection{{\sf NL} (learning in the $\lambda > 0$ case)} \label{subsec:NL1}

We now consider  the case where all loss functions $\varphi_{ij}$ are convex and $\lambda >0$.
\begin{assumption}\label{as:learning-1}
Each $\varphi_{ij}$ is convex, $\lambda>0$.
\end{assumption}

When combined with  Assumption~\ref{as:general}, Assumption~\ref{as:learning-1} implies that $\varphi_{ij}''(t)\geq 0$ for all $t$, hence $h_{ij}(x) = \varphi''_{ij}(a_i^\top x)\geq 0$ for all $x\in \R^d$. In particular, $h_{ij}(x^*) \geq 0$ for all $i,j$. Since we wish to construct a sequence of vectors $h_i^k = (h_{i1}^k, \dots, h_{im}^k)\in \R^m$ satisfying $h_{ij}^k \to h_{ij}(x^*)$, it makes sense to try to enforce all vectors in this sequence to have {\em nonnegative entries}: $$h_{ij}^k\geq 0.$$

 Since  $\mH^k$ arises as a linear combination of the rank-one matrices $a_{ij}a_{ij}^\top$ (see \eqref{eq:Newton2-xx}), this makes  $\mH^k$ positive semidefinite, which in turn means that the matrix $\mH^k + \lambda \mI$ appearing in the main iteration \eqref{eq:Newton-learn} of {\sf NEWTON-LEARN} is invertible, and hence the iteration is {\em well defined.}\footnote{Positive definiteness of Hessian estimates is enforced in several popular quasi-Newton methods as well; for instance, in the BFGS method \cite{Broyden1967, Fletcher1970, Goldfarb1970, shanno1970conditioning}. However, quasi-Newton methods operate in a markedly different manner, and the way in which positive definiteness is enforced there is also different.} 
 
 
 
\subsubsection{The learning iteration and the {\sf NL} algorithm}

 In particular, in {\sf NEWTON-LEARN} each node $i$  computes the vector $\newalpha_i(x^k)\in \R^m$  of second derivatives defined in \eqref{eq:8f0d8hfd}, and then performs the update
\begin{equation}\label{eq: big78fd_8h9fd}\boxed{\quad h^{k+1}_i = \left[h^k_i + \eta \cC_i^k(\newalpha_i(x^k) - h^k_i) \right]_+, \quad}\end{equation}
where $\eta>0$ is a learning rate, $\cC_i^k$ is a freshly sampled compressor by node $i$ at iteration $k$. By $[\cdot ]_+$ we denote the positive part function applied element-wise, defined for scalars as follows: $[t ]_+ = t$ if $t\geq 0$ and  $[t ]_+ = 0$ otherwise. 

We remark that it is possible to interpret the learning procedure \eqref{eq: big78fd_8h9fd}  as one step of projected stochastic gradient descent (SGD)  applied to a certain quadratic optimization problem whose unique solution is the vector $\newalpha_i(x^k)$. 



The {\sf NL} algorithm (Algorithm~\ref{alg:NL1}) arises as the combination of the Newton-like update~\eqref{eq:Newton-learn} (adjusted to take account of the explicit regularizer) and the learning procedure~\eqref{eq: big78fd_8h9fd}. It is easy to see that the update rule for $\mH^k$ in {\sf NL} is designed to ensure that $\mH^k$ remains of the form  $\mH^k = \frac{1}{n}\sum_{i=1}^n \mH^k_i$, where $\mH^k_i = \frac{1}{m} \sum_{j=1}^{m} h^k_{ij} a_{ij}a_{ij}^\top$. The update rule for $x^k$, performed by the server, is identical to~\eqref{eq:Newton-learn},  with an extra provision for the regularizer. The vector $x^{k+1}$ is broadcasted to all workers. Let us comment on how the key communication step is implemented. If the server does not have direct access to the training data vectors $\{a_{ij} \}$, we choose Option~1, otherwise we choose Option~2. A key property of {\sf NL} is that the server is able to maintain  copies of the learning vectors $h_i^k$ {\em without the need for these vectors to be communicated by the workers to the server.} Indeed, provided the workers and the server agree on the same set of initial vectors $h_1^0, \dots, h_n^0$, update~\eqref{eq: big78fd_8h9fd} can be independently computed by  the server as well from its memory state $h_i^k$ and the compressed message $\cC_i^k (\newalpha_i(x^k) - h^k_i)$ received from node $i$. This strategy is reminiscent of the way the key step in the first-order method DIANA~\citep{DIANA, DIANA2} is executed. In this sense, {\sf NL} can be seen as arising from a successful marriage of Newton's method and the DIANA trick.





\begin{algorithm}[tb]
	\caption{{\sf NL: NEWTON-LEARN} ($\lambda>0$ case)}
	\label{alg:NL1}
\begin{algorithmic}
		\STATE {\bfseries Parameters:} learning rate $\eta>0$ 
		\STATE {\bfseries Initialization:}
		$x^0 \in \R^d$; $h^0_1,\dots, h^0_n \in \R^{m}_{+}$; $\mH^0 =  \frac{1}{nm} \sum \limits_{i=1}^n  \sum\limits_{j=1}^{m} h_{ij}^0 a_{ij}a_{ij}^\top\in \R^{d\times d}$
		\FOR{ $k = 0, 1, 2, \dots$}
		\STATE Broadcast $x^k$ to all workers
		\FOR{each node $i = 1, \dots, n$} 
		\STATE Compute local gradient $\nabla f_i(x^k)$ 
		\STATE $h^{k+1}_i = [h^k_i + \eta \cC_i^k (\newalpha_i(x^k) - h^k_i)]_+$ 
		\STATE Send $\nabla f_i(x^k)$ and $\cC_i^k (\newalpha_i(x^k) - h^k_i)$ to server 
		\STATE {\bf Option 1:} Send $\{a_{ij} : h_{ij}^{k+1} - h_{ij}^k \neq 0\}$ to server
		\STATE {\bf Option 2:} Do nothing if server knows $\{a_{ij} : \forall j\}$
		\ENDFOR
		
		\STATE $x^{k+1} = x^k - \left( \mH^k + \lambda \mI \right)^{-1} \left(  \frac{1}{n} \sum\limits_{i=1}^n \nabla f_i(x^k) + \lambda x^k  \right)$
		\STATE $\mH^{k+1} = \mH^k + \frac{1}{nm} \sum \limits_{i=1}^n  \sum\limits_{j=1}^{m} (h_{ij}^{k+1} - h_{ij}^k) a_{ij}a_{ij}^\top $
		\ENDFOR
\end{algorithmic}
\end{algorithm} 

\subsubsection{Theory}

In our theoretical results we rely on the Lyapunov function 
$$
\squeeze \Phi_1^k \eqdef \|x^{k} - x^*\|^2 + \frac{1}{3mn\eta  \nu^2 R^2} {\cal H}^{k}, \qquad {\cal H}^k \eqdef \sum_{i=1}^n \|h_i^k - \newalpha_i(x^*)\|^2. 
$$ Our main theorem follows.

\begin{theorem}[Convergence of {\sf NL}]\label{th:lambda>0}
Let  Assumptions~\ref{as:general} and  \ref{as:learning-1} hold. Let $\eta\leq \frac{1}{\omega+1}$ and assume that $\|x^k - x^*\|^2 \leq \frac{\lambda^2}{12\nu^2R^6}$ for all $k\geq 0$. Then for Algorithm \ref{alg:NL1} we have the inequalities 
\begin{eqnarray*}
\squeeze
\mathbb{E}[\Phi_1^k] & \leq & \theta_1^k \Phi_1^0,\\
\squeeze  
\mathbb{E} \left[  \frac{\|x^{k+1} - x^*\|^2}{\|x^k - x^*\|^2 }  \right] & \leq &\theta_1^k  \left(  {6\eta} + \frac{1}{2}  \right) \frac{\nu^2 R^6}{\lambda^2} \Phi_1^0, 
\end{eqnarray*}
where $\theta_1 \eqdef    1 - \min \left\{  \frac{\eta}{2}, \frac{5}{8}  \right\} $.
\end{theorem}


Since the stepsize bound $\eta \leq \frac{1}{\omega+1}$ is independent of the condition number, the linear convergence rates of $\mathbb{E}[\Phi_1^k]$ and $ \mathbb{E} \left[  \frac{\|x^{k+1} - x^*\|^2}{\|x^k - x^*\|^2 }  \right]$ are both {\em independent of the condition number.} Next, we explore under what conditions we can guarantee for all the iterates to stay in a small neighborhood. 

\begin{lemma}\label{lm:initial-1}
Let Assumptions \ref{as:general} and \ref{as:learning-1} hold. Assume $h_{ij}^k$ is a convex combination of $\{  \newalpha_{ij}(x^0), \newalpha_{ij}(x^1), ..., \newalpha_{ij}(x^k)  \}$ for all $i,j$ and $k$. Assume $\|x^0 - x^*\|^2 \leq \frac{\lambda^2}{12\nu^2R^6}$. Then $$
\|x^k - x^*\|^2 \leq \frac{\lambda^2}{12\nu^2R^6} \quad \text{for all} \quad k\geq 0.
$$ 
\end{lemma}


It is easy to verify that if we choose $h_{ij}^0 = \newalpha_{ij}(x^0)$ and use the random sparsification compressor and $\eta \leq \frac{1}{\omega + 1}$, then $h_{ij}^k$ is always a convex combination of $\{  \newalpha_{ij}(x^0), \newalpha_{ij}(x^1), ..., \newalpha_{ij}(x^k)  \}$ for $k\geq 0$. Thus, from Lemma \ref{lm:initial-1} we can guarantee that all the iterates stay in the small neighborhood assumed in Theorem \ref{th:lambda>0} as long as the initial point $x^0$ is in it.  





\section{Experiments}
\label{sec:experiments}



We now study the empirical performance of our second order method {\sf NL} and compare it with relevant benchmarks and with state-of-the-art methods. We  test on  the regularized logistic regression problem
\begin{eqnarray*}\squeeze
	\min\limits_{x\in\R^d}\left\{\frac{1}{n}\sum\limits_{i=1}^n\frac{1}{m}\sum\limits_{j=1}^m\log\left(1+\exp(-b_{ij}a_{ij}^\top x)\right) + \frac{\lambda}{2}\|x\|^2\right\},
\end{eqnarray*}
where $\{a_{ij}, b_{ij}\}_{j\in[m]}$ are data samples at the $i$-th node. 







\subsection{Data sets and parameter settings} 

In our experiments we use five standard datasets from the LIBSVM library:  {\tt a2a}, {\tt a7a}, {\tt a9a},  {\tt w8a} and {\tt phishing}. Besides, we generated an artificial dataset {\tt artificial} as follows: each of the $d$ elements of the data vector $a_{ij}\in \R^d$ was sampled from the normal distribution $\mathcal{N} (10, 10).$ The corresponding label $b_{ij}$ was sampled uniformly at random from $\{-1, 1\}$. We partitioned each dataset across several nodes (selection of $n$)  in order to capture a variety of scenarios. See Table~\ref{table1} for more details on all the datasets and the choice of $n$. 

In all experiments we use the theoretical parameters (e.g., stepsizes) for all the three algorithms: vanilla Distributed Compressed Gradient Descent (DCGD) \citep{KFJ}, DIANA \citep{DIANA}, and ADIANA \citep{ADIANA}. 


\begin{table}[h]
	\caption{Datasets used in the experiments, and the number of worker nodes $n$ used in each case.}
	\label{table1}
	\begin{center}
		\begin{tabular}{|l|r|r|r|}
			\hline
			 {\bf Datasets} & {\bf \# workers} $n$ &{\bf \# data points} $(=nm)$ & \bf\# features $d$ \\ \hline
			{\tt a2a} & $15$ & $2~265$ & $123$\\ \hline
			{\tt a7a} & $100$ & $16~100$ & $123$\\ \hline
			{\tt a9a} & $80$ & $32~560$ & $123$\\ \hline
			{\tt w8a} & $142$ & $49~700$ & $300$\\ \hline
			{\tt phishing} & $100$ & $11~000$ & $68$\\ \hline
			{\tt artificial} & $100$ & $1~000$ & $200$\\ \hline
		\end{tabular}
	\end{center}
\end{table}


As the initial approximation of the Hessian in BFGS \citep{Broyden1967, Fletcher1970, Goldfarb1970, shanno1970conditioning}, we use $\mH^0~=~\nabla^2P(x^0)$, and the stepsize is $1$. We set the same constants in DINGO \citep{DINGO2019} as they did: $\theta=10^{-4}, \phi=10^{-6}, \rho=10^{-4},$ and use backtracking line search for DINGO to select the largest stepsize in $\{1, 2^{-1}, 2^{-2}, 2^{-4},\dots, 2^{-10}\}$. We conduct experiments for three values of the regularization parameter $\lambda$: $10^{-3}, 10^{-4}, 10^{-5}$.  
In the figures we plot the relation of the optimality gap $P(x^k) - P(x^*)$ and the number of accumulated transmitted bits\footnote{In all plots, ``communicated bits'' refers to the total number of bits sent by all nodes to the server.} or the number of iterations. The optimal value $P(x^*)$ in each case is the function value at the $20$-th iterate of standard Newton's method.  We adopt the realistic setting where the server does not have access to the local data ({\bf Option 1}).



\subsection{Compression operators} For the first order methods we use three compression operators: random sparsification \citep{stich2018sparsified}, random dithering \citep{Alistarh17}, and natural compression \citep{Cnat} (all defined below).
For random-$r$ sparsification, the number of communicated  bits per iteration is $32r+\log_2{\binom{d}{r}}$, and we choose $r = d/4$. For random dithering, we choose $s = \sqrt{d}$, which means the
number of communicated bits per iteration is $2.8d + 32$. For natural compression, the number of communicated bits per iteration is $9d$ bits. 

For {\sf NL} we use the random-$r$ sparsification operator with a selection of values of~$r$.  In addition, we use the random sparsification operator $\cC_p$ induced by the random-$r$ compressor. This compressor is also defined below.




\subsubsection{Random sparsification} The random sparsification compressor \citep{stich2018sparsified}, denoted random-$r$, is a randomized mapping $\cC:\R^m\to \R$ defined as $$\cC(x) \eqdef \frac{m}{r} \cdot \xi \circ x$$ where  $\xi \in \R^m$ is a random vector distributed uniformly at random on the discrete set $\{  y \in \{0, 1 \}^m : \|y\|_0 = r  \}$, where $\|y\|_0\eqdef \{i \;|\; y_i \neq 0\}$ and  $\circ$ is the Hadamard product. The variance  parameter associate with this compressor is $\omega = \frac{m}{r} - 1$. 

\subsubsection{Random dithering} The random dithering compressor \citep{Alistarh17, DIANA2} with $s$ levels is defined via 
$$\cC(x)\eqdef \text{sign}(x)\cdot \|x\|_q\cdot \frac{1}{s} \cdot \xi_s,$$ 
where $\|x\|_q\eqdef \left(\sum_i |x_i|^q\right)^{1/q}$ and $\xi_s\in\R^m$ is a random vector with 
$i$-th element  defined as
$$\xi_s(i)\eqdef \begin{cases}
	l+1 &\text {with probability } \frac{|x_i|}{\|x\|_q}s- l \\
	l &\text {otherwise}
\end{cases}.$$ Here, $l$ satisfies $\frac{|x_i|}{\|x\|_q}\in [\frac{l}{s}, \frac{l+1}{s}]$ and $s \in \mathbb{N}_+$ denotes the levels of the rounding. The variance parameter of this compressor is $\omega \leq 2 + \frac{m^{1/2} + m^{1/q}}{s}$ \citep{DIANA2}. For $q=2$, one can get the improved bound $\omega \leq \min\{  \frac{m}{s^2}, \frac{\sqrt{m}}{s}  \}$ \citep{Alistarh17}.  

\subsubsection{Natural compression} The natural compression  \citep{Cnat} operator $\cC_{\rm nat}:\R^m \to \R$ is obtained by applying the random mapping $\cC_{}:\R\to \R$, defined next, to each coordinate of $x$ independently. We define $\cC(0)=0$ and for $t\neq 0$, we let
$$
\cC(t) \eqdef \begin{cases}
	{\rm sign}(t) \cdot 2^{\lfloor \log_2|t| \rfloor } &\text {with probability } \quad p(t) \eqdef \frac{2^{\lceil \log_2|t| \rceil } - |t|}{2^{\lfloor \log_2|t| \rfloor }} \\
	{\rm sign}(t) \cdot 2^{\lceil \log_2|t| \rceil } &\text {with probability } \quad 1-p(t) 
\end{cases}
$$
The variance parameter of natural compression is $\omega = \frac{1}{8}$. 


\subsubsection{Bernoulli compressor} \label{sec:Bernoulli}
A variant of any compression operator $\cC:\R^m \to \R$ can be constructed as follows:
\begin{equation}\label{eq:Qp}
	\cC_p(x) \eqdef \left\{ \begin{array}{rl}
		\frac{1}{p}\cC(x) & \mbox{ with probability $p$} \\
		0 &\mbox{ otherwise}
	\end{array}, \right.
\end{equation}
where $p\in (0, 1]$ is a probability parameter. It is easy to verify that $\cC_p$ is still a compression operator with variance parameter $\omega_p \eqdef \frac{\omega+1}{p} - 1$, where $\omega$ is the variance parameter of the underlying compressor $\cC$.


\subsection{Behavior of {\sf NL} }

Before we compare our method {\sf NL} with competing baselines,  we investigate how is their performance affected by the choice of the  sparsification parameter $r$ defining the random-$r$ sparsification operator  $\cC$. Likewise, we vary the probability parameter $p$ defining the induced Bernoulli compressor  $\cC_p$.


According to the results summarized in  Figure~\ref{exp:NL1_NL2}, the best performance of {\sf NL} is obtained for $r=1$ and $p=1$. We will use these  parameter settings for {\sf NL} in our subsequent experiments where we compare our method with several baselines and state-of-the-art methods.


\begin{figure}[ht]
	\begin{center}
		\centerline{\begin{tabular}{cccc}
				\includegraphics[width = 0.23 \textwidth]{LogReg/a2a/Lambda=1e-3/a2a_nl1_bits_lmb=0.001.pdf}&
				\includegraphics[width = 0.23 \textwidth]{LogReg/a2a/Lambda=1e-4/a2a_nl1_bits_lmb=0.0001.pdf}&
				\includegraphics[width = 0.23 \textwidth]{LogReg/phishing/Lambda=1e-3/phishing_nl1_bits_lmb=0.001.pdf}&
				\includegraphics[width = 0.23 \textwidth]{LogReg/phishing/Lambda=1e-4/phishing_nl1_bits_lmb=0.0001.pdf}
				\\
				(a) {\tt a2a}, $\lambda=10^{-3}$ &(b) {\tt a2a}, $\lambda=10^{-4}$ & (c) {\tt phishing}, $\lambda=10^{-3}$ &(d) {\tt phishing}, $\lambda=10^{-4}$
		\end{tabular}}
		\caption{Performance of {\sf NL} across a few  values of $r$ defining the random-$r$ compressor, and a few values of $p$ defining the induced Bernoulli compressor $\cC_p$. }
		\label{exp:NL1_NL2}
	\end{center}
	\vskip -0.2in
\end{figure}




\subsection{Comparison of {\sf NL}  with Newton's method}

In our next experiment we compare {\sf NL} using different values of $r$ for random-$r$ compression, with Newton's method; see Figure~\ref{a9a:alg1_alg2}. We clearly see that Newton's method performs better than {\sf NL} in terms of iteration complexity, as expected. However, our methods have better communication efficiency than Newton's method, by {\em several orders of magnitude}. Moreover, we see that the smaller $r$ is, the better {\sf NL} performs in terms of communication complexity. In  Figure~\ref{exp:newton} we perform a similar comparison for several more datasets, but focus on communication complexity only. The conclusions are unchanged: our methods  {\sf NL} have superior performance.

\begin{figure}[t]
	\vskip 0.2in
	\begin{center}
		\begin{tabular}{cccc}
			\includegraphics[width = 0.23 \textwidth]{LogReg/artificial/Lambda=1e-4/NL_newton_iter_art_lmb=0_0001.pdf}&
			\includegraphics[width = 0.23 \textwidth]{LogReg/artificial/Lambda=1e-4/NL_newton_bits_art_lmb=0_0001.pdf}&	
			\includegraphics[width = 0.23 \textwidth]{LogReg/artificial/Lambda=1e-5/NL_newton_iter_art_lmb=0_00001.pdf}&
			\includegraphics[width = 0.23 \textwidth]{LogReg/artificial/Lambda=1e-5/NL_newton_bits_art_lmb=0_00001.pdf}\\
			(a) {\tt artificial} & (b) {\tt artificial} & (c) {\tt artificial} & (d) {\tt artificial}\\
			$\lambda=10^{-4}$ & $\lambda=10^{-4}$ & $\lambda=10^{-5}$ & $\lambda=10^{-5}$\\
		\end{tabular}
		\caption{Comparison of {\sf NL} with Newton's method in terms of iteration complexity for (a), (c); in terms of communication complexity for (b), (d).}
		\label{a9a:alg1_alg2}
	\end{center}
\end{figure}

\begin{figure}[ht]
	\begin{center}
		\centerline{\begin{tabular}{cccc}
				\includegraphics[width = 0.23 \textwidth]{LogReg/w8a/Lambda=1e-3/w8a_nl_newton_bits_lmb=0_001.pdf}&
				\includegraphics[width = 0.23 \textwidth]{LogReg/phishing/Lambda=1e-3/phishing_nl_newton_bits_lmb=0_001.pdf}&
				\includegraphics[width = 0.23 \textwidth]{LogReg/phishing/Lambda=1e-4/phishing_nl_newton_bits_lmb=0_0001.pdf}&
				\includegraphics[width = 0.23 \textwidth]{LogReg/phishing/Lambda=1e-5/phishing_nl_newton_bits_lmb=0_00001.pdf}
				\\
				(a) {\tt w8a}, $\lambda=10^{-3}$ &(b) {\tt phishing} & (c) {\tt phishing} &(d) {\tt phishing}\\
				& $\lambda=10^{-3}$ & $\lambda=10^{-4}$ & $\lambda=10^{-5}$\\
				\includegraphics[width = 0.23 \textwidth]{LogReg/a7a/Lambda=1e-3/a7a_nl_newton_bits_lmb=0_001.pdf}&
				\includegraphics[width = 0.23 \textwidth]{LogReg/a7a/Lambda=1e-4/a7a_nm_nl1_nl2_bits_lmb=0.0001.pdf}&
				\includegraphics[width = 0.23 \textwidth]{LogReg/a2a/Lambda=1e-3/a2a_nl_newton_bits_lmb=0_001.pdf}&
				\includegraphics[width = 0.23 \textwidth]{LogReg/a2a/Lambda=1e-4/a2a_nl_newton_bits_lmb=0_0001.pdf}
				\\
				(e) {\tt a7a}, $\lambda=10^{-3}$ &(f) {\tt a7a}, $\lambda=10^{-3}$ & (g) {\tt a2a}, $\lambda=10^{-3}$ &(h) {\tt a2a}, $\lambda=10^{-4}$
		\end{tabular}}
		\caption{Comparison of {\sf NL} with Newton's method in terms of communication complexity.}
		\label{exp:newton}
	\end{center}
\end{figure}





\subsection{Comparison of {\sf NL} with BFGS}

In our next test, we compare {\sf NL}  with BFGS in Figure~\ref{exp:bfgs}. As we can see, our methods have better communication efficiency than BFGS, by {\em several orders of magnitude}.

\begin{figure}[t]
	\begin{center}
		\centerline{\begin{tabular}{cccc}
				\includegraphics[width = 0.22 \textwidth]{LogReg/a9a/Lambda=1e-3/a9a_nl_bfgs_bits_lmb=0_001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/a9a/Lambda=1e-4/a9a_nl_bfgs_bits_lmb=0_0001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/w8a/Lambda=1e-3/w8a_nl_bfgs_bits_lmb=0_001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/phishing/Lambda=1e-5/phishing_nl_bfgs_bits_lmb=0_00001.pdf}
				\\
				(a) {\tt a9a}, $\lambda=10^{-3}$ &(b) {\tt a9a}, $\lambda=10^{-4}$ & (c) {\tt w8a}, $\lambda=10^{-3}$ &(d) {\tt phishing}, $\lambda=10^{-5}$\\
				\includegraphics[width = 0.22 \textwidth]{LogReg/a7a/Lambda=1e-3/a7a_nl_bfgs_bits_lmb=0_001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/a7a/Lambda=1e-4/a7a_nl_bfgs_bits_lmb=0_0001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/a2a/Lambda=1e-3/a2a_nl_bfgs_bits_lmb=0_001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/a2a/Lambda=1e-4/a2a_nl_bfgs_bits_lmb=0_0001.pdf}
				\\
				(e) {\tt a7a}, $\lambda=10^{-3}$ &(f) {\tt a7a}, $\lambda=10^{-4}$ & (g) {\tt a2a}, $\lambda=10^{-3}$ &(h) {\tt a2a}, $\lambda=10^{-4}$
		\end{tabular}}
		\caption{Comparison of {\sf NL} and BFGS in terms of communication complexity.}
		\label{exp:bfgs}
	\end{center}
\end{figure}








\subsection{Comparison of {\sf NL} with ADIANA}

Next, we compare {\sf NL}with ADIANA~\citep{ADIANA} using three different compression operators: natural compression (DIANA-NC),  random sparsification (DIANA-RS, $r = d/4$) and random dithering (DIANA-RD, $s = \sqrt{d}$); see Figure~\ref{exp:adiana}. Based on the experimental results, we can conclude that {\sf NL} outperforms all three versions ADIANA for all types of compression, often by {\em several degrees of magnitude.}

\begin{figure}[t]
	\begin{center}
		\centerline{\begin{tabular}{cccc}
				\includegraphics[width = 0.22 \textwidth]{LogReg/a9a/Lambda=1e-3/a9a_nl_adiana_bits_lmb=0_001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/a9a/Lambda=1e-4/a9a_nl_adiana_bits_lmb=0_0001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/w8a/Lambda=1e-3/w8a_nl_adiana_bits_lmb=0_001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/phishing/Lambda=1e-5/phishing_nl_adiana_bits_lmb=0_00001.pdf}
				\\
				(a) {\tt a9a}, $\lambda=10^{-3}$ &(b) {\tt a9a}, $\lambda=10^{-4}$ & (c) {\tt w8a}, $\lambda=10^{-3}$ &(d) {\tt phishing}, $\lambda=10^{-5}$\\
				\includegraphics[width = 0.22 \textwidth]{LogReg/a7a/Lambda=1e-3/a7a_nl_adiana_bits_lmb=0_001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/a7a/Lambda=1e-4/a7a_nl_adiana_bits_lmb=0_0001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/phishing/Lambda=1e-3/phishing_nl_adiana_bits_lmb=0_001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/phishing/Lambda=1e-4/phishing_nl_adiana_bits_lmb=0_0001.pdf}
				\\
				(e) {\tt a7a}, $\lambda=10^{-3}$ &(f) {\tt a7a}, $\lambda=10^{-4}$ & (g) {\tt phishing}, $\lambda=10^{-3}$ &(h) {\tt phishing}, $\lambda=10^{-4}$
		\end{tabular}}
		\caption{Comparison of {\sf NL} with ADIANA in terms of communication complexity.}
		\label{exp:adiana}
	\end{center}
\end{figure}






\subsection{Comparison of {\sf NL}  with DINGO}

In our next experiment, we compare {\sf NL} with DINGO~\citep{DINGO2019}. The results, presented  in Figure~\ref{exp:dingo}, show that our method are more communication efficient than DINGO  by {\em many orders of magnitude.} This is true for all  experiments.












\clearpage
\begin{figure}[th]
	\begin{center}
		\centerline{\begin{tabular}{cccc}
				\includegraphics[width = 0.22 \textwidth]{LogReg/a9a/Lambda=1e-3/a9a_nl1_nl2_dingo_lmb=0.001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/a9a/Lambda=1e-4/a9a_nl1_nl2_dingo_lmb=0.0001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/w8a/Lambda=1e-3/w8a_nl1_nl2_dingo_lmb=0.001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/phishing/Lambda=1e-5/phishing_nl1_nl2_dingo_lmb=0.00001.pdf}
				\\
				(a) {\tt a9a}, $\lambda=10^{-3}$ &(b) {\tt a9a}, $\lambda=10^{-4}$ & (c) {\tt w8a}, $\lambda=10^{-3}$ &(d) {\tt phishing}, $\lambda=10^{-5}$\\
				\includegraphics[width = 0.22 \textwidth]{LogReg/a2a/Lambda=1e-3/a2a_nl1_nl2_dingo_lmb=0.001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/a2a/Lambda=1e-4/a2a_nl1_nl2_dingo_lmb=0.0001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/phishing/Lambda=1e-3/phishing_nl1_nl2_dingo_lmb=0.001.pdf}&
				\includegraphics[width = 0.22 \textwidth]{LogReg/phishing/Lambda=1e-4/phishing_nl1_nl2_dingo_lmb=0.0001.pdf}
				\\
				(e) {\tt a2a}, $\lambda=10^{-3}$ &(f) {\tt a2a}, $\lambda=10^{-4}$ & (g) {\tt phishing}, $\lambda=10^{-3}$ &(h) {\tt phishing}, $\lambda=10^{-4}$
		\end{tabular}}
		\caption{Comparison of {\sf NL} with DINGO in terms of communication complexity.}
		\label{exp:dingo}
	\end{center}
\end{figure}



\clearpage

\appendix


\part*{Appendix}



\section{Proofs for {\sf NL} (Section~\ref{subsec:NL1})}


Let $W^k \eqdef (h_1^k, \dots, h_n^k, x^k)$.


\subsection{Lemma}

We first establish a lemma.

\begin{lemma}\label{lm:calHk} Let $S^2\eqdef \sum_{i=1}^n \sum_{j=1}^m \norm{a_{ij}}^2$. For $\eta \leq \frac{1}{\omega+1}$ and all $k\geq 0$, we have 
$$
\ExpBr{ {\cal H}^{k+1} \;|\; W^k} \leq  (1-\eta) {\cal H}^k +  \eta \nu^2 S^2 \norm{x^k - x^*}^2.
$$
\end{lemma}

\begin{proof}
First, recall that
\begin{equation}\label{eq:bu98fg9d_087f98dhf}{\cal H}^{k} =   \sum_{i=1}^n \norm{h_i^{k} - h_i(x^*)}^2 .\end{equation}Since $h_{i}(x^*) \in \R^m_+$ due to convexity of $f_{ij}$, we have \begin{equation} \label{ew:ub98g9fdu_09u9fdf}\norm{[x]_+ - h_i(x^*)}^2 \leq \norm{x - h_i(x^*)}^2, \qquad \text{for all} \qquad x \in \R^m.\end{equation} Then as long as $\eta \leq \frac{1}{\omega+1}$, we have 
\begin{eqnarray}
&&\ExpBr{{\cal H}^{k+1} \;|\; W^k}	= \ExpBr{  \sum_{i=1}^n \norm{h_i^{k+1} - h_i(x^*)}^2  \;|\; W^k} \notag \\ 
	&=&  \sum_{i=1}^n\ExpBr{  \norm{ [h^k_i + \eta \cC_i^k(h_i(x^k) - h^k_i)]_+ - h_i(x^*) }^2  \;|\; W^k} \notag  \\ 
	&\overset{\eqref{ew:ub98g9fdu_09u9fdf}}{\leq} &    \sum_{i=1}^n \ExpBr{\norm{ h^k_i + \eta \cC_i^k(h_i(x^k) - h^k_i) - h_i(x^*) }^2 \;|\; W^k }\notag  \\ 
	&=& \sum_{i=1}^n  \norm{ h_i^k - h_i(x^*) }^2   + 2 \eta\sum_{i=1}^n \ExpBr{  \langle  \cC_i^k(h_i(x^k) - h_i^k),  h_i^k - h_i(x^*) \rangle \;|\; W^k}\notag  \\
	&& \qquad + \eta^2 \sum_{i=1}^n \ExpBr{ \norm{\cC_i^k(h_i(x^k) - h_i^k) }^2 \;|\; W^k} \notag \\ 
	&\overset{\eqref{eq:bu98fg9d_087f98dhf}+\eqref{eq:unbiased} + \eqref{eq:omega-variance}}{\leq} &  {\cal H}^k  + 2\eta \sum_{i=1}^n    \langle  h_i(x^k) - h_i^k, h_i^k - h_i(x^*) \rangle\notag\\
	&& \quad + \eta^2 \sum_{i=1}^n  (\omega+1) \|h_i(x^k) - h_i^k\|^2 .\label{eq:nbu98fd-09ud9f8hufd-fd}
	\end{eqnarray}
	
\noindent Using the stepsize restriction $\eta \leq \frac{1}{\omega+1}$, we can bound $\eta^2 (\omega+1) \leq \eta$. Plugging this back in  \eqref{eq:nbu98fd-09ud9f8hufd-fd}, we get
\begin{eqnarray*}
\ExpBr{{\cal H}^{k+1} \;|\; W^k}
	&\leq &  {\cal H}^k  + \eta  \sum_{i=1}^n   \langle h_i(x^k) - h_i^k, h_i(x^k) + h_i^k - 2h_i(x^*) \rangle  \\ 
	&=&  {\cal H}^k  + \eta \sum_{i=1}^n  \left( \norm{h_i(x^k) - h_i(x^*)}^2 - \norm{h_i^k - h_i(x^*)}^2\right)   \\ 
	&=& (1-\eta) {\cal H}^k  + \eta \sum_{i=1}^n  \norm{h_i(x^k) - h_i(x^*)}^2  \\ 
		&\overset{\eqref{eq:8f0d8hfd}}{=} & (1-\eta) {\cal H}^k  + \eta \sum_{i=1}^n  \sum_{j=1}^m (h_{ij}(x^k) - h_{ij}(x^*))^2  \\ 
	&\overset{(\ref{eq:alphaijL})}{\leq}&  (1-\eta) {\cal H}^k + \eta\sum_{i=1}^n \sum_{j=1}^m \nu^2 \norm{a_{ij}}^2 \norm{x^k - x^*}^2 \\ 
	&\leq&  (1-\eta) {\cal H}^k +  \eta \nu^2 S^2 \norm{x^k - x^*}^2. 
\end{eqnarray*}

\end{proof}

\subsection{Proof of Theorem~\ref{th:lambda>0}}


It is easy to see that \begin{equation}\label{eq:b97gfd-ujo_09op}\mH^k = \frac{1}{n} \sum_{i=1}^n \mH^k_i, \qquad \mH^k_i = \frac{1}{m} \sum_{j=1}^{m} h^k_{ij} a_{ij}a_{ij}^\top .\end{equation}
By the first order necessary optimality conditions, we have
\begin{equation}\label{eq:optx}
\nabla f(x^*) + \lambda x^*  = \nabla P(x^*)= 0. 
\end{equation}
Furthermore, since we maintain $\mH^k \succeq 0$ for all $k$, we have $\mH^k + \lambda \mI \succeq \lambda \mI$, whence  \begin{equation}\label{eq:89f8gd9=08yh98fd}\norm{\left(\mH^k + \lambda \mI \right)^{-1}} \leq \frac{1}{\lambda}.\end{equation}
Then we can write
\begin{eqnarray}
	\norm{ x^{k+1} - x^* } &\overset{\eqref{eq:Newton-learn}}{=}& \norm{ x^k - x^* - (\mH^k + \lambda \mI)^{-1} (\nabla f(x^k) + \lambda x^k) }\notag  \\
	&=& \norm{ (\mH^k + \lambda \mI)^{-1} \left(  (\mH^k + \lambda \mI)(x^k - x^*) - \nabla f(x^k) - \lambda x^k  \right) } \notag \\ 
	&\overset{\eqref{eq:89f8gd9=08yh98fd}}{\leq}& \frac{1}{\lambda} \norm{  (\mH^k + \lambda \mI)(x^k - x^*) - \nabla f(x^k) - \lambda x^k  } \notag \\ 
	&\overset{(\ref{eq:optx})}{=}& \frac{1}{\lambda} \norm{ \mH^k (x^k - x^*) - (\nabla f(x^k) - \nabla f(x^*)) } \notag \\ 
	&=& \frac{1}{\lambda} \norm{ \frac{1}{n} \sum_{i=1}^n \left[\mH_i^k (x^k - x^*) - (\nabla f_i(x^k) - \nabla f_i(x^*)) \right]}\notag  \\ 
	&\leq& \frac{1}{n \lambda} \sum_{i=1}^n \norm{ \mH_i^k (x^k - x^*) - (\nabla f_i(x^k) - \nabla f_i(x^*)) }.\notag
	\end{eqnarray}
	where the last step follows from applying Jensen's inequality to he convex function $x\mapsto \norm{x}$. Using~\eqref{eq:b97gfd-ujo_09op} and \eqref{eq:f_and_f_i} we get
	\begin{eqnarray}
	\norm{x^{k+1}-x^*} \leq \frac{1}{n \lambda} \sum_{i=1}^n \norm{ \frac{1}{m} \sum_{j=1}^m  \left[ h_{ij}^k a_{ij} a_{ij}^\top (x^k - x^*) - (\nabla f_{ij}(x^k) - \nabla f_{ij}(x^*)) \right] }.\label{eq:pop_us_lop_989f}
	\end{eqnarray}
	

\noindent We can now express the difference of the gradients in integral form via the fundamental theorem of calculus, obtaining 
\begin{eqnarray*} \nabla f_{ij}(x^k) - \nabla f_{ij}(x^*) &=&   \int_{0}^1 \mH_{ij} (x^* + \tau(x^k - x^*)) (x^k - x^*) \; d\tau \\
&\overset{\eqref{eq:87ybfd0fd}}{=} & \int_{0}^1 h_{ij}(x^* + \tau(x^k - x^*)) a_{ij}a_{ij}^\top  (x^k - x^*)\; d\tau .\end{eqnarray*}
We can plug this back into \eqref{eq:pop_us_lop_989f}, which gives
\begin{eqnarray}	
&&\norm{ x^{k+1} - x^* }\notag\\
&\leq & 
 \frac{1}{n \lambda} \sum_{i=1}^n \frac{1}{m} \left\|  \sum_{j=1}^{m} \left(  h_{ij}^k a_{ij}a_{ij}^\top (x^k - x^*)-  \int_{0}^1 h_{ij} (x^* + \tau(x^k - x^*))a_{ij}a_{ij}^\top (x^k - x^*)d\tau  \right)   \right\| \notag \\ 
	&=& \frac{1}{n \lambda} \sum_{i=1}^n \frac{1}{m} \left\|  \sum_{j=1}^{m}  a_{ij}a_{ij}^\top (x^k - x^*)\left( h_{ij}^k- \int_{0}^1 h_{ij}(x^* + \tau(x^k - x^*))  d\tau \right) \right\|\notag \\ 
	&\leq& \frac{ \norm{x^k - x^*}}{ \lambda} \frac{1}{nm} \sum_{i=1}^n \sum_{j=1}^{m} \norm{a_{ij}}^2 \left|   \int_{0}^1 h_{ij}^k - h_{ij}(x^* + \tau(x^k - x^*))  d\tau  \right|. \label{petrol-09809fd}
\end{eqnarray}

\noindent From (\ref{eq:alphaijL}), we have 
\begin{eqnarray}
	&&\left|   \int_{0}^1 h_{ij}^k - h_{ij}(x^* + \tau(x^k - x^*))  d\tau  \right|\notag\\
	&\leq& \int_{0}^1 \left|  h_{ij}^k - h_{ij}(x^* + \tau(x^k - x^*))   \right| d\tau \notag \\
	&\leq& |h_{ij}^k - h_{ij}(x^*)| + \int_{0}^1 \left|  h_{ij}(x^*) -  h_{ij}(x^* + \tau(x^k - x^*))   \right| d \tau \notag \\ 
	&\overset{(\ref{eq:alphaijL})}{\leq}&  |h_{ij}^k - h_{ij}(x^*)| + \int_{0}^1 \tau \nu \|a_{ij}\| \cdot \|x^k - x^*\| d\tau  \notag \\ 
	&=&  |h_{ij}^k - h_{ij}(x^*)| + \frac{\nu \|a_{ij}\|}{2} \|x^k-x^*\|. \label{eq:olive-09u0hfdih}
\end{eqnarray}

\noindent By squaring both sides of \eqref{petrol-09809fd},  applying Jensen's inequality to the function $t\mapsto t^2$ in the form $\left(\frac{1}{nm}\sum_i \sum_j t_{ij}\right)^2 \leq \frac{1}{nm}\sum_i \sum_j t^2_{ij}$, and plugging in the bound \eqref{eq:olive-09u0hfdih}, we get
\begin{eqnarray*}	
&&\norm{ x^{k+1} - x^* }^2\\
&\leq&  \frac{ \norm{x^k - x^*}^2}{ \lambda^2} \left(\frac{1}{nm} \sum_{i=1}^n \sum_{j=1}^{m} \norm{a_{ij}}^2 \left|   \int_{0}^1 h_{ij}^k - h_{ij}(x^* + \tau(x^k - x^*))  d\tau  \right| \right)^2 \\
&\leq &\frac{ \norm{x^k - x^*}^2}{ \lambda^2} \frac{1}{nm} \sum_{i=1}^n \sum_{j=1}^{m} \left( \norm{a_{ij}}^2  \left| \int_{0}^1 h_{ij}^k - h_{ij}(x^* + \tau(x^k - x^*))  d\tau  \right|\right)^2 \\
&\overset{\eqref{eq:olive-09u0hfdih}}{\leq}&  \frac{\|x^k-x^*\|^2}{ \lambda^2} \frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^{m} \|a_{ij}\|^4 \left(   |h_{ij}^k - h_{ij}(x^*)| + \frac{\nu \|a_{ij}\|}{2} \|x^k-x^*\|  \right)^2 \\ 
&\leq&  \frac{\|x^k-x^*\|^2}{ \lambda^2} \frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^{m} \|a_{ij}\|^4 \left(   2|h_{ij}^k - h_{ij}(x^*)|^2 + \frac{\nu^2 \|a_{ij}\|^2}{2} \|x^k-x^*\|^2  \right) \\
&\leq & \frac{\|x^k-x^*\|^2}{ \lambda^2} \frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^{m} R^4 \left(   2|h_{ij}^k - h_{ij}(x^*)|^2 + \frac{\nu^2 R^2}{2} \|x^k-x^*\|^2  \right).
\end{eqnarray*}
In the last step we have used Young's inequality.\footnote{$(a+b)^2 \leq 2a^2 + 2b^2$.}


\noindent Since $R\eqdef \max_{i, j} \{ \|a_{ij}\| \}$, we can further bound
\begin{equation}\label{eq:xk+1iter}
\|x^{k+1} - x^*\|^2 \leq \frac{2 R^4}{nm\lambda^2}  \cH^k \|x^k-x^*\|^2 + \frac{ \nu^2 R^6}{2\lambda^2}\|x^k-x^*\|^4. 
\end{equation}


\noindent Assume $\|x^k - x^*\|^2 \leq \frac{\lambda^2}{12\nu^2R^6}$ for all $k\geq 0$. Then from (\ref{eq:xk+1iter}), we have 
\begin{eqnarray*}
	\|x^{k+1} - x^*\|^2 &\leq& \frac{\lambda^2}{12\nu^2 R^6} \cdot \frac{2R^4}{nm\lambda^2} \cH^k + \frac{\lambda^2}{12\nu^2 R^6} \cdot \frac{ \nu^2 R^6\|x^k-x^*\|^2}{2\lambda^2} \\ 
	&\leq& \frac{1}{6nm \nu^2 R^2} {\cal H}^k + \frac{1}{24} \|x^k - x^*\|^2, 
\end{eqnarray*}
and by taking expectation, we have 
\begin{equation}\label{eq:expxk+1}
\ExpBr{ \|x^{k+1} - x^*\|^2 \;|\; W^k } \leq  \frac{1}{6nm \nu^2 R^2}  \cH^k + \frac{1}{24} \|x^k - x^*\|^2. 
\end{equation}
Next, Lemma~\ref{lm:calHk} implies that
\begin{equation}\label{eq:oklah090pop}
\ExpBr{ {\cal H}^{k+1} \;|\; W^k} \leq  (1-\eta) {\cal H}^k +  \eta nm \nu^2 R^2 \norm{x^k - x^*}^2.
\end{equation}

Recall that $\Phi_1^{k+1} \eqdef \|x^{k+1} - x^*\|^2 + \frac{1}{3\eta nm  \nu^2 R^2} {\cal H}^{k+1}$. Combining \eqref{eq:expxk+1} and \eqref{eq:oklah090pop}, we get
\begin{eqnarray}
	\ExpBr{ \Phi_1^{k+1} \;|\; W^k} &=& \ExpBr{ \|x^{k+1} - x^*\|^2 \;|\; W^k} + \frac{1}{3\eta nm  \nu^2 R^2}  \ExpBr{ {\cal H}^{k+1} \;|\; W^k}  \notag\\ 
	&\overset{\eqref{eq:expxk+1}}\leq&  \frac{1}{3\eta nm  \nu^2 R^2} \left(  1 - \eta + \frac{\eta}{2}  \right) {\cal H}^k  + \left(  \frac{1}{24} + \frac{1}{3}  \right)  \|x^k - x^*\|^2  \notag \\ 
	&\leq& \left(  1 - \min \left\{  \frac{\eta}{2}, \frac{5}{8}  \right\}  \right)  \Phi_1^k = \theta_1^k \Phi_1^k. \label{eq:ieopllup-878s}
\end{eqnarray}
By applying the tower property, we get $$\ExpBr{\Phi_1^{k+1}} = \ExpBr{\ExpBr{ \Phi_1^{k+1} \;|\; W^k}} \overset{\eqref{eq:ieopllup-878s}}{\leq} \theta_1 \ExpBr{\Phi_1^k}.$$ Unrolling the recursion, we get   $\ExpBr{\Phi_1^k } \leq  
\theta_1^k  \Phi_1^0$, and the first claim is proved.

We further have $\ExpBr{ \|x^k - x^*\|^2 } \leq \theta_1^k \Phi_1^0$ and $\ExpBr{{\cal H}^k} \leq  \theta_1^k 3\eta nm \nu^2 R^2 \Phi_1^0$. Assume $x^k \neq x^*$ for all $k$. Then from (\ref{eq:xk+1iter}), we have 
$$
\frac{\|x^{k+1} - x^*\|^2}{\|x^k - x^*\|^2 } \leq \frac{2R^4}{nm \lambda^2} {\cal H}^k + \frac{\nu^2 R^6}{2\lambda^2}\|x^k - x^*\|^2, 
$$
and by taking expectation, we obtain 
\begin{eqnarray*}
	\ExpBr{  \frac{\|x^{k+1} - x^*\|^2}{\|x^k - x^*\|^2 }  } &\leq& \frac{2R^4}{mn \lambda^2} \ExpBr{{\cal H}^k } + \frac{\nu^2 R^6}{2\lambda^2} \ExpBr{ \|x^k - x^*\|^2 } \\ 
	&\leq&\theta_1^k  \left(  {6\eta} + \frac{1}{2}  \right) \frac{\nu^2 R^6}{\lambda^2} \Phi_1^0. 
\end{eqnarray*}



\subsection{Proof of Lemma \ref{lm:initial-1}}

We prove this by induction. First, we have $\|x^0 - x^*\|^2 \leq \frac{\lambda^2}{12\nu^2R^6}$ by the assumption. We assume $\|x^k - x^*\|^2 \leq \frac{\lambda^2}{12\nu^2R^6}$ holds for all $k \leq K$. For $k\leq K$, since $h_{ij}^k$ is a convex combination of $\{  \newalpha_{ij}(x^0), \newalpha_{ij}(x^1), ..., \newalpha_{ij}(x^k)  \}$ for all $i,j$, we have 
$$
h_{ij}^k = \sum_{t=0}^k \rho_t \newalpha_{ij}(x^t), \quad {\rm with}\quad  \sum_{t=0}^k\rho_t = 1, \quad {\rm and} \quad \rho_t \geq 0, 
$$
which implies that 
\begin{eqnarray*}
	\sum_{i=1}^n\|h_i^k - \newalpha_i(x^*)\|^2 &=& \sum_{i=1}^n \sum_{j=1}^m |h_{ij}^k - \newalpha_{ij}(x^*)|^2 \\
	&=&  \sum_{i=1}^n \sum_{j=1}^m \left|  \sum_{t=0}^k \rho_t (\newalpha_{ij}(x^t) - \newalpha_{ij}(x^*))  \right|^2 \\ 
	&\leq&  \sum_{i=1}^n \sum_{j=1}^m  \sum_{t=0}^k \rho_t | \newalpha_{ij}(x^t) - \newalpha_{ij}(x^*) |^2 \\ 
	&\overset{(\ref{eq:alphaijL})}{\leq}&  \sum_{i=1}^n \sum_{j=1}^m  \sum_{t=0}^k \rho_t \nu^2 \|a_{ij}\|^2 \|x^t - x^*\|^2 \\ 
	&\overset{\text{As.}~\ref{as:learning-1}}{\leq}& \nu^2 R^2 \sum_{i=1}^n \sum_{j=1}^m  \sum_{t=0}^k \rho_t \cdot \frac{\lambda^2}{12\nu^2R^6} \\
	&=& \frac{mn \lambda^2}{12R^4}, 
\end{eqnarray*}
for $k\leq K$. 

\noindent Combining the above inequality and (\ref{eq:xk+1iter}), we arrive at 
\begin{eqnarray*}
	\|x^{K+1} - x^*\|^2 &\leq& \frac{2\|x^K-x^*\|^2 R^4}{mn\lambda^2} \sum_{i=1}^n \|h_i^K - \newalpha_i(x^*)\|^2 + \frac{ \nu^2 R^6\|x^K-x^*\|^4}{2\lambda^2} \\
	&\leq&  \frac{2\|x^K-x^*\|^2 R^4}{mn\lambda^2} \cdot  \frac{mn \lambda^2}{12R^4} +  \frac{ \nu^2 R^6\|x^K-x^*\|^4}{2\lambda^2} \\
	&\leq& \frac{1}{6}\|x^K - x^*\|^2 +  \frac{ \nu^2 R^6\|x^K-x^*\|^4}{2\lambda^2} \\
	&\leq& \frac{1}{6} \cdot \frac{\lambda^2}{12\nu^2R^6} + \frac{ \nu^2 R^6}{2\lambda^2} \cdot \left(\frac{\lambda^2}{12\nu^2R^6} \right)^2 \\ 
	&\leq& \frac{\lambda^2}{12\nu^2R^6}. 
\end{eqnarray*}







\clearpage




\addcontentsline{toc}{section}{\protect\numberline{}References}
\bibliographystyle{plainnat}
\bibliography{references}


\end{document} 
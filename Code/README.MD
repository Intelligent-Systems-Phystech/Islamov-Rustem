# Code guidelines

This implementation is based on Python (3.8). 

It enables to run simulated distributed optimization with master node on any number of workers. Here you can find implementations of NL1, NL2, CNL, DINGO, BFGS, and gradient type methods (DCGD, DIANA, ADIANA).

### Installation

To install requirements
```sh
$ pip install -r Requirements.txt
```

###  Example Notebooks
To run Newton type methods (NEWTON-LEARN and DINGO) see [example notebook](https://github.com/Rustem-Islamov/MaxNewton/blob/main/Code/NEWTON-LEARN/NL_DINGO_example_notebook.ipynb)     
To run BFGS see [example notebook](https://github.com/Rustem-Islamov/MaxNewton/blob/main/Code/BFGS/BFGS_example_notebook.ipynb)      
To run gradient type methods see [example notebook](https://github.com/Rustem-Islamov/MaxNewton/blob/main/Code/Gradient-type-methods/gradient_type_methods_example_notebook.ipynb)

***Remark*** We used implementation of BFGS which was written by other people.

### License
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

# Code guidelines

This implementation is based on Python (3.8). 

It enables to run simulated distributed optimization with master node on any number of workers. Here you can find implementations of NL1, NL2, CNL, DINGO, BFGS, and gradient type methods (DCGD, DIANA, ADIANA).

### Installation

To install requirements
```sh
$ pip install -r Requirements.txt
```

###  Example Notebooks
To run Newton type methods (NEWTON-LEARN, DINGO) and gradient type methods (DIANA, ADIANA) see [example notebook](https://github.com/Intelligent-Systems-Phystech/Islamov-BS-Thesis/blob/main/Code/Example_notebook.ipynb)     
To run BFGS see [example notebook](https://github.com/Rustem-Islamov/MaxNewton/blob/main/Code/BFGS/BFGS_example_notebook.ipynb)      


### License
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

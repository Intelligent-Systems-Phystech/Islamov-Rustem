{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from oracles import LogReg\n",
    "\n",
    "from methods import Standard_Newton, Newton_Star\n",
    "from methods import NL1, NL1_Bernoulli\n",
    "from methods import NL2, NL2_Bernoulli\n",
    "from methods import CNL, CNL_Bernoulli\n",
    "from methods import DINGO, dcgd\n",
    "from methods import diana, adiana\n",
    "\n",
    "from utils import read_data\n",
    "from utils import loss_logistic, grad, random_k, positive_part\n",
    "from utils import default_dataset_parameters\n",
    "from utils import topK_vectors, biased_rounding, randomK_vectors\n",
    "from utils import Low_Rank, PowerSgdCompression, TopK\n",
    "from utils import pos_projection, semidef_projection\n",
    "from utils import random_spars_matrix, rand_dith\n",
    "\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'a2a'\n",
    "dataset_path = './Datasets/{}.txt'.format(data_name)\n",
    "\n",
    "\n",
    "# regularization parameter\n",
    "lmb = 1e-3\n",
    "\n",
    "# number of nodes, size of local data, and dimension of the problem\n",
    "# according to the paper\n",
    "N = default_dataset_parameters[data_name]['N']# size of the whole data set\n",
    "n = default_dataset_parameters[data_name]['n']# number of nodes\n",
    "m = default_dataset_parameters[data_name]['m']# size of local data set\n",
    "d = default_dataset_parameters[data_name]['d']# dimension of the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, b = read_data(dataset_path=dataset_path, \n",
    "                 N=N, n=n, m=m, d=d, lmb=lmb,\n",
    "                labels=['+1', '-1'])\n",
    "\n",
    "# labels are {+1, -1} for a2a, a7a, a9a, w8a datasets\n",
    "# labels are {0, 1} for phishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the problem \n",
    "logreg = LogReg(A=A, b=b, reg_coef=lmb, n=n, m=m)\n",
    "\n",
    "# find the solution using Newton's method starting from zeros for 20 iterations\n",
    "Newton = Standard_Newton(logreg)\n",
    "Newton.find_optimum(x0=np.zeros(d), n_steps=20,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimum and optimal function value\n",
    "x_opt = logreg.get_optimum()\n",
    "f_opt = logreg.function_value(x_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-embassy",
   "metadata": {},
   "source": [
    "# Newton-Learn 1 (NL1)\n",
    "\n",
    "***input:***\n",
    "- `x` - initial model weightsn\n",
    "- `H` - list of vectors h_i^0\n",
    "- `max_iter` - maximum number of iterations of the method\n",
    "- `k` - the parameter of Rand-K compression operator\n",
    "- `eta` - stepsize for update of vectors h_i's\n",
    "  - if `eta` is None, then `eta` is set as k/m)\n",
    "- `tol` - desired tolerance of the solution\n",
    "- `init_cost` \n",
    "  - if True, then the communication cost of initalization is inclued\n",
    "- `verbose` \n",
    "  - if True, then function values in each iteration are printed\n",
    "\n",
    "return:\n",
    "- `func_value` - numpy array containing function value in each iteration of the method\n",
    "- `bits` - numpy array containing transmitted bits by one node to the server\n",
    "- `iterates` - numpy array containing distances from current point to the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial point \n",
    "x = x_opt + np.ones(d)*0.1\n",
    "\n",
    "# define the method\n",
    "nl1 = NL1(logreg)\n",
    "\n",
    "# define vectors h_i's as second derivatives at the initial point\n",
    "H = []\n",
    "for i in range(n):\n",
    "    H.append(logreg.alphas(x,i))\n",
    "\n",
    "# run the method\n",
    "fv, bi, it = nl1.method(x=x, H=H, init_cost=False, k=1, eta=1/m,\n",
    "                        tol=1e-15, max_iter=20, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-newcastle",
   "metadata": {},
   "source": [
    "## Newton-Learn 1 (NL1) with induced Bernoulli compressor \n",
    "\n",
    "All parameters are the same as above, but one more parameter `p` is included as a parameter of Bernoulli parameter. Note that here communicated bits per node computed without paying attention on Bernoulli compressor. If you want to make fair comparison, you need to calculate bits sended by all \"active\" nodes (multiply by $pn$).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial point \n",
    "x = x_opt + np.ones(d)*0.1\n",
    "\n",
    "# define the method\n",
    "nl1_b = NL1_Bernoulli(logreg)\n",
    "\n",
    "# define vectors h_i's as second derivatives at the initial point\n",
    "H = []\n",
    "for i in range(n):\n",
    "    H.append(logreg.alphas(x,i))\n",
    "\n",
    "# run the method\n",
    "fv, bi, it = nl1_b.method(x=x, p=1/2, H=H, init_cost=False, k=1, eta=1/m,\n",
    "                          tol=1e-15, max_iter=20, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-minimum",
   "metadata": {},
   "source": [
    "# Newton-Learn 2 (NL2)\n",
    "\n",
    "***input:***\n",
    "- `x` - initial model weightsn\n",
    "- `H` - list of vectors h_i^0\n",
    "- `gamma` - parameter of NL2\n",
    "- `max_iter` - maximum number of iterations of the method\n",
    "- `k` - the parameter of Rand-K compression operator\n",
    "- `eta` - stepsize for update of vectors h_i's\n",
    "  - if `eta` is None, then `eta` is set as k/m)\n",
    "- `tol` - desired tolerance of the solution\n",
    "- `init_cost` \n",
    "  - if True, then the communication cost of initalization is inclued\n",
    "- `verbose` \n",
    "  - if True, then function values in each iteration are printed\n",
    "\n",
    "return:\n",
    "- `func_value` - numpy array containing function value in each iteration of the method\n",
    "- `bits` - numpy array containing transmitted bits by one node to the server\n",
    "- `iterates` - numpy array containing distances from current point to the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial point \n",
    "x = x_opt + np.ones(d)*0.1\n",
    "\n",
    "# define the method\n",
    "nl2 = NL2(logreg)\n",
    "\n",
    "# define vectors h_i's as second derivatives at the initial point\n",
    "H = []\n",
    "for i in range(n):\n",
    "    H.append(logreg.alphas(x,i))\n",
    "\n",
    "# run the method\n",
    "fv, bi, it = nl2.method(x=x, H=H, gamma=0.5, init_cost=False, k=1, eta=1/m,\n",
    "                        tol=1e-15, max_iter=20, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-complement",
   "metadata": {},
   "source": [
    "## Newton-Learn 2 (NL2) with induced Bernoulli compressor \n",
    "\n",
    "All parameters are the same as above, but one more parameter `p` is included as a parameter of Bernoulli parameter. Note that here communicated bits per node computed without paying attention on Bernoulli compressor. If you want to make fair comparison, you need to calculate bits sended by all \"active\" nodes (multiply by $pn$).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial point \n",
    "x = x_opt + np.ones(d)*0.1\n",
    "\n",
    "# define the method\n",
    "nl2_b = NL2_Bernoulli(logreg)\n",
    "\n",
    "# define vectors h_i's as second derivatives at the initial point\n",
    "H = []\n",
    "for i in range(n):\n",
    "    H.append(logreg.alphas(x,i))\n",
    "\n",
    "# run the method\n",
    "fv, bi, it = nl2_b.method(x=x, H=H, gamma=0.5, p=1/2, init_cost=False, k=1, eta=1/m,\n",
    "                        tol=1e-15, max_iter=20, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-multimedia",
   "metadata": {},
   "source": [
    "# Cubic-Newton-Learn (CNL)\n",
    "\n",
    "***input:***\n",
    "- `x` - initial model weightsn\n",
    "- `H` - list of vectors h_i^0\n",
    "- `nu`, `gamma` - parameteres of CNL\n",
    "- `max_iter` - maximum number of iterations of the method\n",
    "- `k` - the parameter of Rand-K compression operator\n",
    "- `eta` - stepsize for update of vectors h_i's\n",
    "  - if `eta` is None, then `eta` is set as k/m)\n",
    "- `tol` - desired tolerance of the solution\n",
    "- `init_cost` \n",
    "  - if True, then the communication cost of initalization is inclued\n",
    "- `verbose` \n",
    "  - if True, then function values in each iteration are printed\n",
    "\n",
    "return:\n",
    "- `func_value` - numpy array containing function value in each iteration of the method\n",
    "- `bits` - numpy array containing transmitted bits by one node to the server\n",
    "- `iterates` - numpy array containing distances from current point to the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial point \n",
    "x = x_opt + np.ones(d)*1\n",
    "\n",
    "# define the method\n",
    "cnl = CNL(logreg)\n",
    "\n",
    "# define vectors h_i's as second derivatives at the initial point\n",
    "H = []\n",
    "for i in range(n):\n",
    "    H.append(np.zeros(m))\n",
    "    #H.append(logreg.alphas(x,i))\n",
    "\n",
    "# run the method\n",
    "fv, bi, it = cnl.method(x=x, H=H, nu=1/8, gamma=0.5, init_cost=False, k=1, eta=1/m,\n",
    "                        tol=1e-15, max_iter=20, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-fishing",
   "metadata": {},
   "source": [
    "## Cubic-Newton-Leanr (CNL) with induced Bernoulli compressor\n",
    "\n",
    "All parameters are the same as above, but one more parameter `p` is included as a parameter of Bernoulli parameter. Note that here communicated bits per node computed without paying attention on Bernoulli compressor. If you want to make fair comparison, you need to calculate bits sended by all \"active\" nodes (multiply by $pn$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial point \n",
    "x = x_opt + np.ones(d)*1\n",
    "\n",
    "# define the method\n",
    "cnl_b = CNL_Bernoulli(logreg)\n",
    "\n",
    "# define vectors h_i's as second derivatives at the initial point\n",
    "H = []\n",
    "for i in range(n):\n",
    "    H.append(np.zeros(m))\n",
    "    #H.append(logreg.alphas(x,i))\n",
    "\n",
    "# run the method\n",
    "fv, bi, it = cnl_b.method(x=x, H=H, nu=1/8, gamma=0.5, init_cost=False, k=1, eta=1/m,\n",
    "                        tol=1e-15, max_iter=20, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-approval",
   "metadata": {},
   "source": [
    "# Newton-Star\n",
    "\n",
    "***input:***\n",
    "- `x` - initial model weightsn\n",
    "- `init_cost` - if True, then the communication cost of initalization is inclued\n",
    "- `tol` - desired tolerance of the solution\n",
    "- `max_iter` - maximum number of iterations\n",
    "- `verbose` - if True, then function values in each iteration are printed\n",
    "\n",
    "***return:***\n",
    "- `func_value` - numpy array containing function value in each iteration of the method\n",
    "- `bits` - numpy array containing transmitted bits by one node to the server\n",
    "- `iterates` - numpy array containing distances from current point to the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial point \n",
    "x = x_opt + np.ones(d)*0.2\n",
    "\n",
    "# define the method\n",
    "n0 = Newton_Star(logreg)\n",
    "\n",
    "# run the method\n",
    "fv, bi, it = n0.method(x0=x, init_cost=False,\n",
    "                      tol=1e-15, max_iter=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-luxury",
   "metadata": {},
   "source": [
    "# DINGO\n",
    "\n",
    "***input:***\n",
    "- `x` - initial point\n",
    "- `max_iter` - maximum iterations of the method\n",
    "- `tol` - desired tolerance of the solution\n",
    "- `phi`, `theta`, `rho` - parameters of DINGO\n",
    "- `verbose` - if True, then function values in each iteration are printed\n",
    "\n",
    "***return:***\n",
    "- `func_value` - numpy array containing function value in each iteration of the method\n",
    "- `bits` - numpy array containing transmitted bits by one node to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial point\n",
    "x = x_opt + np.ones(d)*0.1\n",
    "\n",
    "# define method\n",
    "dingo = DINGO(logreg)\n",
    "\n",
    "# run the method\n",
    "fv, bi = dingo.method(x=x, phi=1e-6, theta=1e-4, \n",
    "                      rho=1e-4, tol=1e-15, \n",
    "                      max_iter=20, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-integral",
   "metadata": {},
   "source": [
    "# DCGD, ADIANA and DIANA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eta, alpha, theta_1, theta_2, gamma are parameters of ADIANA \n",
    "class args(EasyDict):\n",
    "    def __init__(self, data_name, T, node, L, lamda, eta=0.05,\n",
    "                 alpha=0.5, theta_1=0.25, theta_2=0.5, gamma=0.5,\n",
    "                 beta=0.95,\n",
    "                 prob=1,\n",
    "                 comp_method='no_comp',\n",
    "                 r=None, s=None, s_level=10):\n",
    "        super().__init__()\n",
    "        self.data_name = data_name\n",
    "        # T - maximum number of iterations\n",
    "        self.T = T\n",
    "        # number of nodes\n",
    "        self.node = node\n",
    "        # parameters of ADIANA\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.theta_1 = theta_1\n",
    "        self.theta_2 = theta_2\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.prob = prob\n",
    "        # Lipshitz constant of gradients\n",
    "        self.L = L\n",
    "        # regularization parameter\n",
    "        self.lamda = lamda\n",
    "        # name of compression operator\n",
    "        self.comp_method = comp_method\n",
    "        # parameter of random sparsification\n",
    "        # compression operator\n",
    "        self.r = r\n",
    "        # parameter of randim dithering\n",
    "        # compression operator\n",
    "        self.s = s\n",
    "    \n",
    "# find Lipshitz constant of gradients\n",
    "H = np.dot(A.T,A)/N\n",
    "temp = np.linalg.eigvalsh(H)\n",
    "L = np.abs(temp[-1])/4\n",
    "\n",
    "# define maximum number of iterations\n",
    "max_iter = 200\n",
    "\n",
    "# set class containing all parameters\n",
    "arg = args(data_name, max_iter, n, L, lmb)\n",
    "\n",
    "# set parameters of compression operators\n",
    "arg.r = d/4\n",
    "arg.s = np.sqrt(d)\n",
    "\n",
    "# set compression operator\n",
    "comp_methods = [\n",
    "    'rand_dithering',\n",
    "    'rand_sparse',\n",
    "    'natural_comp'\n",
    "]\n",
    "\n",
    "# put compression operator to arg's\n",
    "arg.comp_method = comp_methods[0]\n",
    "\n",
    "# initial point\n",
    "x = x_opt + np.ones(d)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dcgd, com_bits_dcgd = dcgd(A, b, x, arg, f_opt=f_opt, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ADIANA\n",
    "loss_adiana, com_bits_adiana = adiana(A, b, x, arg, f_opt=f_opt, tol=1e-15)\n",
    "\n",
    "# method prints function values each 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ADIANA\n",
    "\n",
    "loss_diana, com_bits_diana = diana(A, b, x, arg, f_opt=f_opt, tol=1e-15)\n",
    "# method prints function values each 1000 iterations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
